{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24e5bd18",
   "metadata": {},
   "source": [
    "# Code Perturbation using Gemma\n",
    "\n",
    "This notebook analyzes the dataset, discards code snippets longer than 4096 tokens, and processes the entire dataset to create perturbations using a Gemma model by Google\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "763b9d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3070\n",
      "VRAM: 8.0 GB\n"
     ]
    }
   ],
   "source": [
    "# Local Model Setup for Code Modification\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c8884c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from data.aigcodeset import AIGCodeSet\n",
    "import logging\n",
    "from datasets import concatenate_datasets\n",
    "from Levenshtein import distance as Levenshtein_distance\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6c2f00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/gemma-2b-it\"  # Using 2B for better performance on most hardware\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3672f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "train, val, test = AIGCodeSet(cache_dir='../../data').get_dataset(split=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f568ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size: 7583\n",
      "First sample target: 1\n",
      "First sample code length: 229 characters\n"
     ]
    }
   ],
   "source": [
    "# Concatenate and explore dataset\n",
    "dataset = concatenate_datasets([train, val, test])\n",
    "print(f\"Total dataset size: {len(dataset)}\")\n",
    "print(f\"First sample target: {dataset[0]['target']}\")\n",
    "print(f\"First sample code length: {len(dataset[0]['code'])} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d4f942",
   "metadata": {},
   "source": [
    "##### Place all code samples inside of prompt, tokenize and save the ones that exceed about half+headroom of the model's 8192 token output context window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b930c16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''<start_of_turn>user\n",
    "Refine this Python code to be more concise and readable while preserving its exact functionality. The code's input and output behavior must remain unchanged, regardless of input types or structure. Ensure the refined code:\n",
    "- Is valid Python with no syntax errors.\n",
    "- Maintains the same logic and results as the original.\n",
    "- Avoids introducing new dependencies or invalid assumptions (e.g., treating scalars as iterables).\n",
    "Return only the refined code, no explanations or comments:\n",
    "\n",
    "{code}\n",
    "<end_of_turn>\n",
    "<start_of_turn>model'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f0f0c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_lengths = {}\n",
    "\n",
    "\n",
    "for i, sample in enumerate(dataset['code']):\n",
    "    input_text = prompt.format(code=sample)\n",
    "    tokens = tokenizer(input_text, return_length=True)  \n",
    "    sample_lengths[i] = tokens['length'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65117e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 192), (1, 263), (2, 1131), (3, 193), (4, 232)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sample_lengths.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce8ac075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA90AAAJOCAYAAACqS2TfAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAbHhJREFUeJzt3Xl8TPf+x/H3ZF+YWLNVEIlaStVya2uoSoVQN0VdpbW0KLVTRRdbW7sWbVFt79XbvVy1llatqYa6cVGK0tpKFkUyCAmT8/vDL1MjaIZMJ8vr+XjkcWe+3++c+ZyZMb3vOed8vybDMAwBAAAAAIB85+bqAgAAAAAAKKoI3QAAAAAAOAmhGwAAAAAAJyF0AwAAAADgJIRuAAAAAACchNANAAAAAICTELoBAAAAAHASQjcAAAAAAE5C6AYAAAAAwEkI3QBQzFSuXFk9e/Z0dRkoRB588EE9+OCDri6jQDCZTBo4cKCry7Azfvx4mUymv+S5rv8sbNy4USaTSYsXL/5Lnr9nz56qXLnyX/JcAJBfCN0AUED88ssveuaZZ1SlShX5+PjIbDaradOmmj17ti5evOjq8m7bkSNHZDKZbH/u7u6qWLGiHn30Ue3cudPV5d22SZMmaenSpXkef+rUKQ0ZMkTVq1eXr6+vAgMDdf/992vUqFE6f/688wotgHr27KkSJUq4uoyb+v777zV+/HilpaX95c+9cOFCu38vPj4+Cg0NVUxMjObMmaNz587ly/OcPHlS48ePL5D/BgtybQBwOzxcXQAAQFq1apUee+wxeXt7q3v37qpVq5aysrL03XffaeTIkdq7d68WLFjg6jLvyOOPP67Y2FhZrVbt27dP8+bN0+rVq7V161bdd999ri7PYZMmTVKnTp0UFxf3p2PPnDmjBg0ayGKx6KmnnlL16tV1+vRp7d69W/PmzVP//v0LdAgtbr7//ntNmDBBPXv2VKlSpVxSw8SJExUeHq7Lly8rOTlZGzdu1NChQ/X6669r+fLluvfee21jX3rpJY0ePdqh7Z88eVITJkxQ5cqVHfr398033zj0PLfjVrW9++67ys7OdnoNAJCfCN0A4GKHDx9Wly5dVKlSJa1fv14hISG2vgEDBujQoUNatWqVCyu8uYULF6pXr14yDONPx9arV09PPPGE7X7Tpk3Vvn17zZs3T++8884NH3PhwgX5+/vnW72u8v777+vYsWPasmWLmjRpYtdnsVjk5eXlospQULVp00YNGjSw3R8zZozWr1+vdu3aqX379tq3b598fX0lSR4eHvLwcO7/pcvIyJCfn5/LP6uenp4ufX4AuB2cXg4ALjZt2jSdP39e77//vl3gzhEZGakhQ4bY7l+5ckWvvPKKIiIi5O3trcqVK+uFF15QZmam3eMMw9Crr76qChUqyM/PTy1atNDevXtvWENaWpqGDh2qsLAweXt7KzIyUlOnTnXqEaWHHnpI0tUfHaQ/TqvdtGmTnn32WQUGBqpChQq28XPnztU999wjb29vhYaGasCAAblO/33wwQdVq1Yt7d69W82bN5efn58iIyNt15tu2rRJDRs2lK+vr6pVq6Zvv/3W7vE518bu379fnTt3ltlsVtmyZTVkyBBdunTJNs5kMunChQv64IMPbKcB3+o6+V9++UXu7u5q1KhRrj6z2SwfHx/b/fj4eD322GOqWLGivL29FRYWpmHDhuW6xCDnFO1jx46pXbt2KlGihO666y69/fbbkqQff/xRDz30kPz9/VWpUiV98skndo/Peb03b96sZ555RmXLlpXZbFb37t119uzZm+5LjszMTI0bN06RkZG2Op9//vlcn8M7sW3bNrVu3VoBAQHy8/NT8+bNtWXLFrsxOe/ZoUOHbEemAwIC1KtXL2VkZNiNvXjxogYPHqxy5cqpZMmSat++vU6cOCGTyaTx48fbtjdy5EhJUnh4uO39PXLkiN22li5dqlq1asnb21v33HOP1qxZY9d/7tw5DR06VJUrV5a3t7cCAwP18MMPa8eOHbf9ejz00EN6+eWXdfToUX300Ue5XoNrrV27Vg888IBKlSqlEiVKqFq1anrhhRckXb0O+29/+5skqVevXrZ9XLhwoaQ//h0lJiaqWbNm8vPzsz32Ztf3W61WvfDCCwoODpa/v7/at2+v48eP24252XwS127zz2q70TXdFy5c0IgRI2zfX9WqVdOMGTNy/RiYcz3+n713AJDfCN0A4GIrVqxQlSpVch0BvZnevXtr7Nixqlevnt544w01b95ckydPVpcuXezGjR07Vi+//LLq1Kmj6dOnq0qVKmrVqpUuXLhgNy4jI0PNmzfXRx99pO7du2vOnDlq2rSpxowZo+HDh+fbfl7vl19+kSSVLVvWrv3ZZ5/VTz/9pLFjx9pOmR0/frwGDBig0NBQzZw5Ux07dtQ777yjVq1a6fLly3aPP3v2rNq1a6eGDRtq2rRp8vb2VpcuXfT555+rS5cuio2N1ZQpU3ThwgV16tTphtfIdu7cWZcuXdLkyZMVGxurOXPmqG/fvrb+Dz/8UN7e3oqKitKHH36oDz/8UM8888xN97VSpUqyWq368MMP//R1WbRokTIyMtS/f3+9+eabiomJ0Ztvvqnu3bvnGmu1WtWmTRuFhYVp2rRpqly5sgYOHKiFCxeqdevWatCggaZOnaqSJUuqe/futh84rjVw4EDt27dP48ePV/fu3fXxxx8rLi7ulmcvZGdnq3379poxY4YeeeQRvfnmm4qLi9Mbb7yhf/zjH3+6j3mxfv16NWvWTBaLRePGjdOkSZOUlpamhx56SD/88EOu8Z07d9a5c+c0efJkde7cWQsXLtSECRPsxvTs2VNvvvmmYmNjNXXqVPn6+qpt27Z2Yzp06KDHH39ckvTGG2/Y3t/y5cvbxnz33Xd69tln1aVLF02bNk2XLl1Sx44ddfr0aduYfv36ad68eerYsaPmzp2r5557Tr6+vtq3b98dvS5PPvmkpFuf5r137161a9dOmZmZmjhxombOnKn27dvbfrCoUaOGJk6cKEnq27evbR+bNWtm28bp06fVpk0b3XfffZo1a5ZatGhxy7pee+01rVq1SqNGjdLgwYO1du1aRUdHOzwfRV5qu5ZhGGrfvr3eeOMNtW7dWq+//rqqVaumkSNH3vD7Ky/vHQDkOwMA4DLp6emGJOPvf/97nsbv3LnTkGT07t3brv25554zJBnr1683DMMwUlNTDS8vL6Nt27ZGdna2bdwLL7xgSDJ69Ohha3vllVcMf39/4+eff7bb5ujRow13d3fj2LFjN63nX//6l/Fn/yk5fPiwIcmYMGGCcerUKSM5OdnYuHGjUbduXUOS8Z///MduWw888IBx5coV2+Nz9qVVq1aG1Wq1tb/11luGJOOf//ynra158+aGJOOTTz6xte3fv9+QZLi5uRlbt261tX/99deGJONf//qXrW3cuHGGJKN9+/Z2+/Dss88akoxdu3bZ2vz9/e1ex1tJTk42ypcvb0gyqlevbvTr18/45JNPjLS0tFxjMzIycrVNnjzZMJlMxtGjR21tPXr0MCQZkyZNsrWdPXvW8PX1NUwmk/HZZ5/leg3GjRtna8t5vevXr29kZWXZ2qdNm2ZIMpYtW2Zra968udG8eXPb/Q8//NBwc3Mz4uPj7eqcP3++IcnYsmXLLV+PHj16GP7+/jftz87ONqpWrWrExMTYfX4zMjKM8PBw4+GHH7a15bxnTz31lN02Hn30UaNs2bK2+4mJiYYkY+jQoXbjevbsmeu1mT59uiHJOHz4cK7aJBleXl7GoUOHbG27du0yJBlvvvmmrS0gIMAYMGDAzV+Em8h5X7Zv337TMQEBAUbdunVt93NegxxvvPGGIck4derUTbexffv2XJ//HDn/jubPn3/Dvms/Cxs2bDAkGXfddZdhsVhs7V988YUhyZg9e7atrVKlSjf8N3P9Nm9VW48ePYxKlSrZ7i9dutSQZLz66qt24zp16mSYTCa79ymv7x0A5DeOdAOAC1ksFklSyZIl8zT+q6++kqRcR3BGjBghSbZrv7/99ltlZWVp0KBBdqedDh06NNc2Fy1apKioKJUuXVq///677S86OlpWq1WbN2+2jT179qzdmJxZt69t+/3333Od1itJ48aNU/ny5RUcHKwHH3xQv/zyi6ZOnaoOHTrYjevTp4/c3d1t93P2ZejQoXJzc7MbZzabc13vXqJECbuj/tWqVVOpUqVUo0YNNWzY0Naec/vXX3/NVeuAAQPs7g8aNEjSH6+/o4KCgrRr1y7169dPZ8+e1fz589W1a1cFBgbqlVdesTuqnHOdrnT1tNnff/9dTZo0kWEY+t///pdr271797bdLlWqlKpVqyZ/f3917tzZ1p7zGtxoX/v27Wt3nWz//v3l4eFxy31dtGiRatSooerVq9u97zmXDGzYsCGPr8yN7dy5UwcPHlTXrl11+vRp2/YvXLigli1bavPmzbkufejXr5/d/aioKJ0+fdr2byznFOJnn33WblzOe+uI6OhoRURE2O7fe++9MpvNdq9vqVKltG3bNp08edLh7f+ZEiVK3HIW85zJ35YtW3bbl4h4e3urV69eeR7fvXt3u++xTp06KSQk5Lb/zeTVV199JXd3dw0ePNiufcSIETIMQ6tXr7Zrz8t7BwD5jYnUAMCFzGazJOV5GaCjR4/Kzc1NkZGRdu3BwcEqVaqUjh49ahsnSVWrVrUbV758eZUuXdqu7eDBg9q9e7fd6bPXSk1Ntd2uW7eubdvXb/da48aNs10jm6Nv37567LHH5ObmplKlStmuz75eeHi43f2c56tWrZpdu5eXl6pUqZKrngoVKuS6vjUgIEBhYWG52iTd8Prl61+3iIgIubm55bqu1xEhISGaN2+e5s6dq4MHD+rrr7/W1KlTNXbsWIWEhNjC87FjxzR27FgtX748V23p6el29318fHK99gEBATd9DfKyryVKlFBISMgt9/XgwYPat29fnj4zt+PgwYOSpB49etx0THp6ut1nuWLFinb9OX1nz56V2Wy2/du5/vN1/b+lvLj+uXKe79rXd9q0aerRo4fCwsJUv359xcbGqnv37qpSpYrDz3e98+fPKzAw8Kb9//jHP/Tee++pd+/eGj16tFq2bKkOHTqoU6dOdj9c3cpdd93l0KRp13+OTCaTIiMj7+jfTF4cPXpUoaGhuX64rFGjhq3/Wnl57wAgvxG6AcCFzGazQkNDtWfPHoced32guhPZ2dl6+OGH9fzzz9+w/+6777bd/vjjj+2u0fzmm280ffp0rV271u4xNwoWVatWVXR09J/Wc+2R3ttx7VHyvLQbeZh5PT9fb5PJpLvvvlt333232rZtq6pVq+rjjz9W7969ZbVa9fDDD+vMmTMaNWqUqlevLn9/f504cUI9e/bMddTSGfuaF9nZ2apdu7Zef/31G/Zf/wPH7WxfkqZPn37T5ayuX2LN2fvs6HN17txZUVFR+vLLL23/TqZOnaolS5aoTZs2t/3cv/32m9LT02/5Y4Gvr682b96sDRs2aNWqVVqzZo0+//xzPfTQQ/rmm29uWv/128hvN/t3ZLVa81RTfvgrPycAkIPQDQAu1q5dOy1YsEAJCQlq3LjxLcdWqlRJ2dnZOnjwoO1IjiSlpKQoLS1NlSpVso2Trh4xvDYAnzp1KtcRnYiICJ0/fz5Pgbhp06Z293/77TdJytNjb1fOvhw4cMBuX7KysnT48GGnPPfBgwftjogeOnRI2dnZdrMm50cQr1KlikqXLq2kpCRJV2cc//nnn/XBBx/YTZx2/Y8a+engwYN2k2SdP39eSUlJio2NveljIiIitGvXLrVs2TJff5C4dvvS1R+l8uv9zfm3c/jwYbujsocOHco1Nr/2KSQkRM8++6yeffZZpaamql69enrttdfuKHTnTMYXExNzy3Fubm5q2bKlWrZsqddff12TJk3Siy++qA0bNig6Ojrf37ecsxNyGIahQ4cO2a0nXrp06VwrDkhXj0Zf+2/bkdoqVaqkb7/9VufOnbM72r1//35bPwC4Gtd0A4CLPf/88/L391fv3r2VkpKSq/+XX37R7NmzJckWhGbNmmU3JueIY85MzNHR0fL09NSbb75pdwTn+sdJV4/IJSQk6Ouvv87Vl5aWpitXrtzWfuWX6OhoeXl5ac6cOXb78v777ys9PT3X7NP5IWfZrRxvvvmmJNmFJX9//xsGiBvZtm1brlnjJemHH37Q6dOnbafO5xyFu3Y/DcOwvf/OsGDBArsZ4OfNm6crV67cMhh27txZJ06c0Lvvvpur7+LFizfcV0fUr19fERERmjFjhm3egGudOnXK4W3mhNS5c+fatee8t9fKWRs+r+/v9axWa65LAQIDAxUaGnpHS6qtX79er7zyisLDw9WtW7ebjjtz5kyutpwzBnKe/0738Xr//ve/7S6TWbx4sZKSkuw+RxEREdq6dauysrJsbStXrsy1tJgjtcXGxspqteqtt96ya3/jjTdkMpnu6AcOAMgvHOkGABeLiIjQJ598on/84x+qUaOGunfvrlq1aikrK0vff/+9Fi1aZFvbtk6dOurRo4cWLFigtLQ0NW/eXD/88IM++OADxcXF2Y5Yli9fXs8995wmT56sdu3aKTY2Vv/73/+0evVqlStXzu75R44cqeXLl6tdu3bq2bOn6tevrwsXLujHH3/U4sWLdeTIkVyP+SuVL19eY8aM0YQJE9S6dWu1b99eBw4c0Ny5c/W3v/1NTzzxRL4/5+HDh9W+fXu1bt1aCQkJ+uijj9S1a1fVqVPHNqZ+/fr69ttv9frrrys0NFTh4eF2E7Vd68MPP9THH3+sRx99VPXr15eXl5f27dunf/7zn/Lx8bGtgVy9enVFREToueee04kTJ2Q2m/Wf//zHqdebZmVlqWXLlurcubPtdX3ggQfUvn37mz7mySef1BdffKF+/fppw4YNatq0qaxWq/bv368vvvhCX3/9tRo0aHDL5718+bJeffXVXO1lypTRs88+q/fee09t2rTRPffco169eumuu+7SiRMntGHDBpnNZq1YscKh/axfv746duyoWbNm6fTp02rUqJE2bdqkn3/+WZL90dX69etLkl588UV16dJFnp6eeuSRR2xh8M+cO3dOFSpUUKdOnVSnTh2VKFFC3377rbZv366ZM2fmaRurV6/W/v37deXKFaWkpGj9+vVau3atKlWqpOXLl9ut7X69iRMnavPmzWrbtq0qVaqk1NRUzZ07VxUqVNADDzwg6er3TqlSpTR//nyVLFlS/v7+atiwYa5r3vOqTJkyeuCBB9SrVy+lpKRo1qxZioyMVJ8+fWxjevfurcWLF6t169bq3LmzfvnlF3300Ud2E5s5WtsjjzyiFi1a6MUXX9SRI0dUp04dffPNN1q2bJmGDh2aa9sA4BIumTMdAJDLzz//bPTp08eoXLmy4eXlZZQsWdJo2rSp8eabbxqXLl2yjbt8+bIxYcIEIzw83PD09DTCwsKMMWPG2I0xDMOwWq3GhAkTjJCQEMPX19d48MEHjT179txw2Z5z584ZY8aMMSIjIw0vLy+jXLlyRpMmTYwZM2bYLSd1PUeWDJs+ffotx/3ZUklvvfWWUb16dcPT09MICgoy+vfvb5w9e9ZuTPPmzY177rkn12MrVapktG3bNle7JLtlnXKWXvrpp5+MTp06GSVLljRKly5tDBw40Lh48aLdY/fv3280a9bM8PX1zbUM2/V2795tjBw50qhXr55RpkwZw8PDwwgJCTEee+wxY8eOHXZjf/rpJyM6OtooUaKEUa5cOaNPnz62ZY2uXULpZstu5fU1yHm9N23aZPTt29coXbq0UaJECaNbt27G6dOnc23z2iWdDMMwsrKyjKlTpxr33HOP4e3tbZQuXdqoX7++MWHCBCM9Pf2mr0VO7ZJu+BcREWEb97///c/o0KGDUbZsWcPb29uoVKmS0blzZ2PdunW2MTnv2fXLY+Xs37XLfl24cMEYMGCAUaZMGaNEiRJGXFycceDAAUOSMWXKFLvHv/LKK8Zdd91luLm52W3n+s/Mta9vzmcgMzPTGDlypFGnTh2jZMmShr+/v1GnTh1j7ty5t3xdrq0758/Ly8sIDg42Hn74YWP27Nl2y3Jd/xrkWLdunfH3v//dCA0NNby8vIzQ0FDj8ccfz7Us4LJly4yaNWsaHh4edp+vm32GcvputGTYp59+aowZM8YIDAw0fH19jbZt29otcZdj5syZxl133WV4e3sbTZs2Nf773//e8PN1s9quXzLMMK5+fw0bNswIDQ01PD09japVqxrTp0+3W27OMPL23gGAM5gMg5kjAACQpPHjx2vChAk6deqUS4/u/xUWLlyoXr16afv27X96VLoo27lzp+rWrauPPvrolqdsAwBwu7imGwAAFAvXzryfY9asWXJzc1OzZs1cUBEAoDjgmm4AAFAsTJs2TYmJiWrRooU8PDy0evVqrV69Wn379r3jZc4AALgZQjcAACgWmjRporVr1+qVV17R+fPnVbFiRY0fP14vvviiq0sDABRhLj29fPPmzXrkkUcUGhoqk8mkpUuX2vUbhqGxY8cqJCREvr6+io6OzrUO5JkzZ9StWzeZzWaVKlVKTz/9dK7lRXbv3q2oqCj5+PgoLCxM06ZNy1XLokWLVL16dfn4+Kh27dr66quv8n1/AQAF2/jx42UYRpG/nluSevbsKcMwitX13A8//LC+++47nTlzRllZWTp06JDGjRsnDw+OQQAAnMelofvChQuqU6dOrvVQc0ybNk1z5szR/PnztW3bNvn7+ysmJkaXLl2yjenWrZv27t2rtWvXauXKldq8ebP69u1r67dYLGrVqpUqVaqkxMRETZ8+XePHj9eCBQtsY77//ns9/vjjevrpp/W///1PcXFxiouL0549e5y38wAAAACAIq/AzF5uMpn05ZdfKi4uTtLVo9yhoaEaMWKEnnvuOUlSenq6goKCtHDhQnXp0kX79u1TzZo17WZeXbNmjWJjY/Xbb78pNDRU8+bN04svvqjk5GR5eXlJkkaPHq2lS5dq//79kqR//OMfunDhglauXGmrp1GjRrrvvvs0f/78v/BVAAAAAAAUJQX2fKrDhw8rOTlZ0dHRtraAgAA1bNhQCQkJ6tKlixISElSqVCm7U+Oio6Pl5uambdu26dFHH1VCQoKaNWtmC9ySFBMTo6lTp+rs2bMqXbq0EhISNHz4cLvnj4mJyXW6+7UyMzOVmZlpu5+dna0zZ86obNmyMplM+fAKAAAAAAAKKsMwdO7cOYWGhsrN7eYnkRfY0J2cnCxJCgoKsmsPCgqy9SUnJyswMNCu38PDQ2XKlLEbEx4enmsbOX2lS5dWcnLyLZ/nRiZPnqwJEybcxp4BAAAAAIqK48ePq0KFCjftL7Chu6AbM2aM3dHx9PR0VaxYUcePH5fZbHZhZQAAAAAAZ7NYLAoLC1PJkiVvOa7Ahu7g4GBJUkpKikJCQmztKSkpuu+++2xjUlNT7R535coVnTlzxvb44OBgpaSk2I3Juf9nY3L6b8Tb21ve3t652s1mM6EbAAAAAIqJP7u82KWzl99KeHi4goODtW7dOlubxWLRtm3b1LhxY0lS48aNlZaWpsTERNuY9evXKzs7Ww0bNrSN2bx5sy5fvmwbs3btWlWrVk2lS5e2jbn2eXLG5DwPAAAAAAC3w6Wh+/z589q5c6d27twp6erkaTt37tSxY8dkMpk0dOhQvfrqq1q+fLl+/PFHde/eXaGhobYZzmvUqKHWrVurT58++uGHH7RlyxYNHDhQXbp0UWhoqCSpa9eu8vLy0tNPP629e/fq888/1+zZs+1ODR8yZIjWrFmjmTNnav/+/Ro/frz++9//auDAgX/1SwIAAAAAKEJcumTYxo0b1aJFi1ztPXr00MKFC2UYhsaNG6cFCxYoLS1NDzzwgObOnau7777bNvbMmTMaOHCgVqxYITc3N3Xs2FFz5sxRiRIlbGN2796tAQMGaPv27SpXrpwGDRqkUaNG2T3nokWL9NJLL+nIkSOqWrWqpk2bptjY2Dzvi8ViUUBAgNLT0zm9HAAAAACKuLxmwAKzTndhR+gGAAAACjer1Wp3WSqKN09PT7m7u9+0P68ZsMBOpAYAAAAAfwXDMJScnKy0tDRXl4ICplSpUgoODv7TydJuhdANAAAAoFjLCdyBgYHy8/O7o4CFosEwDGVkZNhWy7p2RS1HEboBAAAAFFtWq9UWuMuWLevqclCA+Pr6SpJSU1MVGBh4y1PNb6XALhkGAAAAAM6Wcw23n5+fiytBQZTzubiTa/0J3QAAAACKPU4px43kx+eC0A0AAAAAgJMQugEAAAAABcKRI0dkMpm0c+dOV5eSbwjdAAAAAFDITZkyRSaTSUOHDrVrv3TpkgYMGKCyZcuqRIkS6tixo1JSUuzGrFu3Tk2aNFHJkiUVHBysUaNG6cqVK3ZjDMPQjBkzdPfdd8vb21t33XWXXnvttZvWUxTD8+0idAMAAABAIbZ9+3a98847uvfee3P1DRs2TCtWrNCiRYu0adMmnTx5Uh06dLD179q1S7GxsWrdurX+97//6fPPP9fy5cs1evRou+0MGTJE7733nmbMmKH9+/dr+fLluv/++52+b0UBoRsAAAAACqnz58+rW7duevfdd1W6dGm7vvT0dL3//vt6/fXX9dBDD6l+/fr617/+pe+//15bt26VJH3++ee69957NXbsWEVGRqp58+aaNm2a3n77bZ07d06StG/fPs2bN0/Lli1T+/btFR4ervr16+vhhx++aV3h4eGSpLp168pkMunBBx+UJGVnZ2vixImqUKGCvL29dd9992nNmjU33Y7VatVTTz2l6tWr69ixY5KkZcuWqV69evLx8VGVKlU0YcIEuyPzJpNJ7733nh599FH5+fmpatWqWr58ua3/7Nmz6tatm8qXLy9fX19VrVpV//rXvxx41R1D6AYAAACAQmrAgAFq27atoqOjc/UlJibq8uXLdn3Vq1dXxYoVlZCQIEnKzMyUj4+P3eN8fX116dIlJSYmSpJWrFihKlWqaOXKlQoPD1flypXVu3dvnTlz5qZ1/fDDD5Kkb7/9VklJSVqyZIkkafbs2Zo5c6ZmzJih3bt3KyYmRu3bt9fBgwdzbSMzM1OPPfaYdu7cqfj4eFWsWFHx8fHq3r27hgwZop9++knvvPOOFi5cmOtU9wkTJqhz587avXu3YmNj1a1bN1u9L7/8sn766SetXr3a9oNCuXLl/vS1vl2EbgAAAAC4kddflypUuPO/jRvtt7tx4x99r79+2+V99tln2rFjhyZPnnzD/uTkZHl5ealUqVJ27UFBQUpOTpYkxcTE6Pvvv9enn34qq9WqEydOaOLEiZKkpKQkSdKvv/6qo0ePatGiRfr3v/+thQsXKjExUZ06dbppbeXLl5cklS1bVsHBwSpTpowkacaMGRo1apS6dOmiatWqaerUqbrvvvs0a9Ysu8efP39ebdu21alTp7Rhwwbb9iZMmKDRo0erR48eqlKlih5++GG98soreuedd+we37NnTz3++OOKjIzUpEmTdP78edsPAceOHVPdunXVoEEDVa5cWdHR0XrkkUf+7OW+bR5O2zIAAAAAFGYWi3TixJ1vJzMz9/2c7Vost7XJ48ePa8iQIVq7dm2uI9WOaNWqlaZPn65+/frpySeflLe3t15++WXFx8fLze3qMdrs7GxlZmbq3//+t+6++25J0vvvv6/69evrwIEDqlatWp6ey2Kx6OTJk2ratKlde9OmTbVr1y67tscff1wVKlTQ+vXr5evra2vftWuXtmzZYndk22q16tKlS8rIyJCfn58k2V3f7u/vL7PZrNTUVElS//791bFjR+3YsUOtWrVSXFycmjRpkteXzGGEbgAAAAC4EbNZuuuuO9+Ot3fu+znbNZtva5OJiYlKTU1VvXr1bG1Wq1WbN2/WW2+9pczMTAUHBysrK0tpaWl2R7tTUlIUHBxsuz98+HANGzZMSUlJKl26tI4cOaIxY8aoSpUqkqSQkBB5eHjYArck1ahRQ9LVo8Z5Dd2OiI2N1UcffaSEhAQ99NBDtvbz589rwoQJdpPB5bj2xwdPT0+7PpPJpOzsbElSmzZtdPToUX311Vdau3atWrZsqQEDBmjGjBn5vh8SoRsAAAAAbmz48Kt/+e3BB6XffrujTbRs2VI//vijXVuvXr1UvXp1jRo1Su7u7qpfv748PT21bt06dezYUZJ04MABHTt2TI0bN7Z7rMlkUmhoqCTp008/VVhYmC3QN23aVFeuXNEvv/yiiIgISdLPP/8sSapUqdIN6/Py8pJ09YeAHGazWaGhodqyZYuaN29ua9+yZUuumdD79++vWrVqqX379lq1apVtfL169XTgwAFFRkY68GrlVr58efXo0UM9evRQVFSURo4cSegGAAAAAFxVsmRJ1apVy67N399fZcuWtbUHBATo6aef1vDhw1WmTBmZzWYNGjRIjRs3VqNGjWyPmz59ulq3bi03NzctWbJEU6ZM0RdffCF3d3dJUnR0tOrVq6ennnpKs2bNUnZ2tgYMGKCHH37Y7uj3tQIDA+Xr66s1a9aoQoUK8vHxUUBAgEaOHKlx48YpIiJC9913n/71r39p586d+vjjj3NtY9CgQbJarWrXrp1Wr16tBx54QGPHjlW7du1UsWJFderUSW5ubtq1a5f27NmjV199NU+v3dixY1W/fn3dc889yszM1MqVK21H7p2B0A0AAAAARdQbb7whNzc3dezYUZmZmYqJidHcuXPtxqxevVqvvfaaMjMzVadOHS1btkxt2rSx9bu5uWnFihUaNGiQmjVrJn9/f7Vp00YzZ8686fN6eHhozpw5mjhxosaOHauoqCht3LhRgwcPVnp6ukaMGKHU1FTVrFlTy5cvV9WqVW+4naFDhyo7O1uxsbFas2aNYmJitHLlSk2cOFFTp06Vp6enqlevrt69e+f5NfHy8tKYMWN05MgR+fr6KioqSp999lmeH+8ok2EYhtO2XoxYLBYFBAQoPT1d5tu8LgMAAADAX+vSpUs6fPiwwsPD72hCMhRNt/p85DUDsmQYAAAAAABOQugGAAAAAMBJCN0AAAAAADgJoRsAAAAAACchdAMAAAAA4CSEbgAAAADFXnZ2tqtLQAGUH58L1ukGAAAAUGx5eXnJzc1NJ0+eVPny5eXl5SWTyeTqsuBihmEoKytLp06dkpubm7y8vG57W4RuAAAAAMWWm5ubwsPDlZSUpJMnT7q6HBQwfn5+qlixotzcbv8kcUI3AAAAgGLNy8tLFStW1JUrV2S1Wl1dDgoId3d3eXh43PGZD4RuAAAAAMWeyWSSp6enPD09XV0KihgmUgMAAAAAwEkI3QAAAAAAOAmhGwAAAAAAJyF0AwAAAADgJIRuAAAAAACchNANAAAAAICTELoBAAAAAHASQjcAAAAAAE5C6AYAAAAAwEkI3QAAAAAAOAmhGwAAAAAAJyF0AwAAAADgJIRuAAAAAACchNANAAAAAICTELoBAAAAAHASQjcAAAAAAE5C6AYAAAAAwEkI3QAAAAAAOAmhGwAAAAAAJyF0AwAAAADgJIRuAAAAAACchNANAAAAAICTELoBAAAAAHASQjcAAAAAAE5C6AYAAAAAwEkI3QAAAAAAOAmhGwAAAAAAJyF0AwAAAADgJIRuAAAAAACchNANAAAAAICTELoBAAAAAHASQjcAAAAAAE5C6AYAAAAAwEkI3QAAAAAAOAmhGwAAAAAAJyF0AwAAAADgJIRuAAAAAACchNANAAAAAICTELoBAAAAAHASQjcAAAAAAE5C6AYAAAAAwEkI3QAAAAAAOAmhGwAAAAAAJyF0AwAAAADgJIRuAAAAAACchNANAAAAAICTELoBAAAAAHASQjcAAAAAAE5C6AYAAAAAwEkI3QAAAAAAOAmhGwAAAAAAJyF0AwAAAADgJIRuAAAAAACchNANAAAAAICTELoBAAAAAHASQjcAAAAAAE5C6AYAAAAAwEkI3QAAAAAAOAmhGwAAAAAAJyF0AwAAAADgJIRuAAAAAACchNANAAAAAICTELoBAAAAAHASQjcAAAAAAE5C6AYAAAAAwEkI3QAAAAAAOEmBDt1Wq1Uvv/yywsPD5evrq4iICL3yyisyDMM2xjAMjR07ViEhIfL19VV0dLQOHjxot50zZ86oW7duMpvNKlWqlJ5++mmdP3/ebszu3bsVFRUlHx8fhYWFadq0aX/JPgIAAAAAiq4CHbqnTp2qefPm6a233tK+ffs0depUTZs2TW+++aZtzLRp0zRnzhzNnz9f27Ztk7+/v2JiYnTp0iXbmG7dumnv3r1au3atVq5cqc2bN6tv3762fovFolatWqlSpUpKTEzU9OnTNX78eC1YsOAv3V8AAAAAQNFiMq49bFzAtGvXTkFBQXr//fdtbR07dpSvr68++ugjGYah0NBQjRgxQs8995wkKT09XUFBQVq4cKG6dOmiffv2qWbNmtq+fbsaNGggSVqzZo1iY2P122+/KTQ0VPPmzdOLL76o5ORkeXl5SZJGjx6tpUuXav/+/Xmq1WKxKCAgQOnp6TKbzfn8SgAAAAAACpK8ZsACfaS7SZMmWrdunX7++WdJ0q5du/Tdd9+pTZs2kqTDhw8rOTlZ0dHRtscEBASoYcOGSkhIkCQlJCSoVKlStsAtSdHR0XJzc9O2bdtsY5o1a2YL3JIUExOjAwcO6OzZszesLTMzUxaLxe4PAAAAAIBrebi6gFsZPXq0LBaLqlevLnd3d1mtVr322mvq1q2bJCk5OVmSFBQUZPe4oKAgW19ycrICAwPt+j08PFSmTBm7MeHh4bm2kdNXunTpXLVNnjxZEyZMyIe9BAAAAAAUVQX6SPcXX3yhjz/+WJ988ol27NihDz74QDNmzNAHH3zg6tI0ZswYpaen2/6OHz/u6pIAAAAAAAVMgT7SPXLkSI0ePVpdunSRJNWuXVtHjx7V5MmT1aNHDwUHB0uSUlJSFBISYntcSkqK7rvvPklScHCwUlNT7bZ75coVnTlzxvb44OBgpaSk2I3JuZ8z5nre3t7y9va+850EAAAAABRZBfpId0ZGhtzc7Et0d3dXdna2JCk8PFzBwcFat26drd9isWjbtm1q3LixJKlx48ZKS0tTYmKibcz69euVnZ2thg0b2sZs3rxZly9fto1Zu3atqlWrdsNTywEAAAAAyIsCHbofeeQRvfbaa1q1apWOHDmiL7/8Uq+//roeffRRSZLJZNLQoUP16quvavny5frxxx/VvXt3hYaGKi4uTpJUo0YNtW7dWn369NEPP/ygLVu2aODAgerSpYtCQ0MlSV27dpWXl5eefvpp7d27V59//rlmz56t4cOHu2rXAQAAAABFQIFeMuzcuXN6+eWX9eWXXyo1NVWhoaF6/PHHNXbsWNtM44ZhaNy4cVqwYIHS0tL0wAMPaO7cubr77rtt2zlz5owGDhyoFStWyM3NTR07dtScOXNUokQJ25jdu3drwIAB2r59u8qVK6dBgwZp1KhRea6VJcMAAAAAoPjIawYs0KG7MCF0AwAAAEDxUSTW6QYAAAAAoDAjdAMAAAAA4CSEbgAAAAAAnITQDQAAAACAkxC6AQAAAABwEkI3AAAAAABOQugGAAAAAMBJCN0AAAAAADgJoRsAAAAAACchdAMAAAAA4CSEbgAAAAAAnITQDQAAAACAkxC6AQAAAABwEkI3AAAAAABOQugGAAAAAMBJCN0AAAAAADgJoRsAAAAAACchdAMAAAAA4CSEbgAAAAAAnITQDQAAAACAkxC6AQAAAABwEkI3AAAAAABOQugGAAAAAMBJCN0AAAAAADgJoRsAAAAAACchdAMAAAAA4CSEbgAAAAAAnITQDQAAAACAkxC6AQAAAABwEkI3AAAAAABOQugGAAAAAMBJCN0AAAAAADgJoRsAAAAAACchdAMAAAAA4CSEbgAAAAAAnITQDQAAAACAkxC6AQAAAABwEkI3AAAAAABOQugGAAAAAMBJCN0AAAAAADgJoRsAAAAAACchdAMAAAAA4CSEbgAAAAAAnITQDQAAAACAkxC6AQAAAABwEkI3AAAAAABOQugGAAAAAMBJPBx9wOHDhxUfH6+jR48qIyND5cuXV926ddW4cWP5+Pg4o0YAAAAAAAqlPIfujz/+WLNnz9Z///tfBQUFKTQ0VL6+vjpz5ox++eUX+fj4qFu3bho1apQqVarkzJoBAAAAACgU8hS669atKy8vL/Xs2VP/+c9/FBYWZtefmZmphIQEffbZZ2rQoIHmzp2rxx57zCkFAwAAAABQWJgMwzD+bNDXX3+tmJiYPG3w9OnTOnLkiOrXr3/HxRUmFotFAQEBSk9Pl9lsdnU5AAAAAAAnymsGzNOR7rwGbkkqW7asypYtm+fxAAAAAAAUVQ7PXr5jxw79+OOPtvvLli1TXFycXnjhBWVlZeVrcQAAAAAAFGYOh+5nnnlGP//8syTp119/VZcuXeTn56dFixbp+eefz/cCAQAAAAAorBwO3T///LPuu+8+SdKiRYvUrFkzffLJJ1q4cKH+85//5Hd9AAAAAAAUWg6HbsMwlJ2dLUn69ttvFRsbK0kKCwvT77//nr/VAQAAAABQiDkcuhs0aKBXX31VH374oTZt2qS2bdtKkg4fPqygoKB8LxAAAAAAgMLK4dA9a9Ys7dixQwMHDtSLL76oyMhISdLixYvVpEmTfC8QAAAAAIDCKk/rdOfFpUuX5O7uLk9Pz/zYXKHDOt0AAAAAUHzk6zrdN5KVlaXU1FTb9d05KlaseLubBAAAAACgSHE4dP/88896+umn9f3339u1G4Yhk8kkq9Wab8UBAAAAAFCYORy6e/XqJQ8PD61cuVIhISEymUzOqAsAAAAAgELP4dC9c+dOJSYmqnr16s6oBwAAAACAIsPh2ctr1qzJetwAAAAAAOSBw6F76tSpev7557Vx40adPn1aFovF7g8AAAAAAFzl8JJhbm5Xc/r113IX94nUWDIMAAAAAIoPpy0ZtmHDhjsqDAAAAACA4sLh0N28eXNn1AEAAAAAQJHj8DXdkhQfH68nnnhCTZo00YkTJyRJH374ob777rt8LQ4AABRsWVlZmjVrlgYNGqRZs2YpKyvL1SUBAFCgOBy6//Of/ygmJka+vr7asWOHMjMzJUnp6emaNGlSvhcIAAAKpueff17+/v4aNmyY3nrrLQ0bNkz+/v56/vnnXV0aAAAFhsOh+9VXX9X8+fP17rvvytPT09betGlT7dixI1+LAwAABdPzzz+v6dOnq2zZsnr33XeVlJSkd999V2XLltX06dMJ3gAA/D+HZy/38/PTTz/9pMqVK6tkyZLatWuXqlSpol9//VU1a9bUpUuXnFVrgcbs5QCA4iIrK0v+/v4qW7asfvvtN3l4/DFFzJUrV1ShQgWdPn1aFy5ckJeXlwsrBQDAefKaAR0+0h0cHKxDhw7lav/uu+9UpUoVRzcHAAAKmblz5+rKlSt69dVX7QK3JHl4eGjixIm6cuWK5s6d66IKAQAoOBwO3X369NGQIUO0bds2mUwmnTx5Uh9//LGee+459e/f3xk1AgCAAuSXX36RJLVr1+6G/TntOeMAACjOHF4ybPTo0crOzlbLli2VkZGhZs2aydvbW88995wGDRrkjBoBAEABEhERIUlauXKlevfunat/5cqVduMAACjOHL6m+/Lly/L09FRWVpYOHTqk8+fPq2bNmipRooR+//13lStXzlm1Fmhc0w0AKC64phsAACde092lSxcZhiEvLy/VrFlT999/v0qUKKGUlBQ9+OCDd1IzAAAoBLy8vDRs2DClpKSoQoUKWrBggU6ePKkFCxaoQoUKSklJ0bBhwwjcAADoNk4vP3bsmHr37q3333/f1paUlKSHHnpI99xzT74WBwAACqZp06ZJkt544w0988wztnYPDw+NHDnS1g8AQHHn8Onlp06dUrNmzdSmTRu9/vrrOnnypFq0aKE6deros88+k5ubwwfPiwROLwcAFEdZWVmaO3eufvnlF0VEROjZZ5/lCDcAoFjIawZ0+Eh3+fLl9c033+iBBx6QdHWylHr16unjjz8utoEbAIDiysvLS0OHDnV1GQAAFFgOh25JCgsL09q1axUVFaWHH35YH374oUwmU37XBgAAAABAoZan0F26dOkbhuqMjAytWLFCZcuWtbWdOXMm/6oDAAAAAKAQy1PonjVrlpPLAAAAAACg6MlT6O7Ro4ez6wAAAAAAoMi5rWu6rVarli5dqn379kmS7rnnHrVv317u7u75WhwAAAAAAIWZw6H70KFDio2N1YkTJ1StWjVJ0uTJkxUWFqZVq1YpIiIi34sEAAAAAKAwcniNr8GDBysiIkLHjx/Xjh07tGPHDh07dkzh4eEaPHiwM2oEAAAAAKBQcvhI96ZNm7R161aVKVPG1la2bFlNmTJFTZs2zdfiAAAAAAAozBw+0u3t7a1z587laj9//ry8vLzypSgAAAAAAIoCh0N3u3bt1LdvX23btk2GYcgwDG3dulX9+vVT+/btnVEjAAAAAACFksOhe86cOYqIiFDjxo3l4+MjHx8fNW3aVJGRkaznDQAAAADANRy+prtUqVJatmyZDh06ZFsyrEaNGoqMjMz34gAAAAAAKMwcPtI9ceJEZWRkKDIyUo888ogeeeQRRUZG6uLFi5o4caIzagQAAAAAoFByOHRPmDBB58+fz9WekZGhCRMm5EtR1zpx4oSeeOIJlS1bVr6+vqpdu7b++9//2voNw9DYsWMVEhIiX19fRUdH6+DBg3bbOHPmjLp16yaz2axSpUrp6aefzrUPu3fvVlRUlHx8fBQWFqZp06bl+74AAAAAAIoXh0O3YRgymUy52nft2mW3jFh+OHv2rJo2bSpPT0+tXr1aP/30k2bOnKnSpUvbxkybNk1z5szR/PnztW3bNvn7+ysmJkaXLl2yjenWrZv27t2rtWvXauXKldq8ebP69u1r67dYLGrVqpUqVaqkxMRETZ8+XePHj9eCBQvydX8AAAAAAMWLyTAMIy8DS5cuLZPJpPT0dJnNZrvgbbVadf78efXr109vv/12vhU3evRobdmyRfHx8TfsNwxDoaGhGjFihJ577jlJUnp6uoKCgrRw4UJ16dJF+/btU82aNbV9+3Y1aNBAkrRmzRrFxsbqt99+U2hoqObNm6cXX3xRycnJtmXPRo8eraVLl2r//v15qtVisSggIMD2+gAAAAAAiq68ZsA8T6Q2a9YsGYahp556ShMmTFBAQICtz8vLS5UrV1bjxo3vrOrrLF++XDExMXrssce0adMm3XXXXXr22WfVp08fSdLhw4eVnJys6Oho22MCAgLUsGFDJSQkqEuXLkpISFCpUqVsgVuSoqOj5ebmpm3btunRRx9VQkKCmjVrZrfOeExMjKZOnaqzZ8/aHVkHAAAAACCv8hy6e/ToIUkKDw9X06ZN5eHh8MTnDvv11181b948DR8+XC+88IK2b9+uwYMHy8vLSz169FBycrIkKSgoyO5xQUFBtr7k5GQFBgba9Xt4eKhMmTJ2Y8LDw3NtI6fvRqE7MzNTmZmZtvsWi+UO9xYAAAAAUNTk6ZruCxcu2G43b978TwP3tePvRHZ2turVq6dJkyapbt266tu3r/r06aP58+fny/bvxOTJkxUQEGD7CwsLc3VJAAAAAIACJk+hOzIyUlOmTFFSUtJNxxiGobVr16pNmzaaM2dOvhQXEhKimjVr2rXVqFFDx44dkyQFBwdLklJSUuzGpKSk2PqCg4OVmppq13/lyhWdOXPGbsyNtnHtc1xvzJgxSk9Pt/0dP378dnYRAAAAAFCE5ekc8Y0bN+qFF17Q+PHjVadOHTVo0EChoaHy8fHR2bNn9dNPPykhIUEeHh4aM2aMnnnmmXwprmnTpjpw4IBd288//6xKlSpJunqqe3BwsNatW6f77rtP0tXTvLdt26b+/ftLkho3bqy0tDQlJiaqfv36kqT169crOztbDRs2tI158cUXdfnyZXl6ekqS1q5dq2rVqt30em5vb295e3vny34CAAAAAIqmPM9eLknHjh3TokWLFB8fr6NHj+rixYsqV66c6tatq5iYGLVp00bu7u75Vtz27dvVpEkTTZgwQZ07d9YPP/ygPn36aMGCBerWrZskaerUqZoyZYo++OADhYeH6+WXX9bu3bv1008/ycfHR5LUpk0bpaSkaP78+bp8+bJ69eqlBg0a6JNPPpF0dcbzatWqqVWrVho1apT27Nmjp556Sm+88Ybd0mK3wuzlAAAAAFB85DUDOhS6XWHlypUaM2aMDh48qPDwcA0fPtw2e7l09bT2cePGacGCBUpLS9MDDzyguXPn6u6777aNOXPmjAYOHKgVK1bIzc1NHTt21Jw5c1SiRAnbmN27d2vAgAHavn27ypUrp0GDBmnUqFF5rpPQDQAAAADFR5EJ3YUFoRsAAAAAio+8ZsA8TaQGAAAAAAAcR+gGAAAAAMBJCN0AAAAAADgJoRsAAAAAACfJ0zrd10tLS9MPP/yg1NRUZWdn2/V17949XwoDAAAAAKCwczh0r1ixQt26ddP58+dlNptlMplsfSaTidANAAAAAMD/c/j08hEjRuipp57S+fPnlZaWprNnz9r+zpw544waAQAAAAAolBwO3SdOnNDgwYPl5+fnjHoAAAAAACgyHA7dMTEx+u9//+uMWgAAAAAAKFLydE338uXLbbfbtm2rkSNH6qefflLt2rXl6elpN7Z9+/b5WyEAAAAAAIWUyTAM488Gubnl7YC4yWSS1Wq946IKI4vFooCAAKWnp8tsNru6HAAAAACAE+U1A+bpSPf1y4IBAAAAAIA/5/A13f/+97+VmZmZqz0rK0v//ve/86UoAAAAAACKgjydXn4td3d3JSUlKTAw0K799OnTCgwM5PRyTi8HAAAAgCIvrxnQ4SPdhmHIZDLlav/tt98UEBDg6OYAAAAAACiy8nRNtyTVrVtXJpNJJpNJLVu2lIfHHw+1Wq06fPiwWrdu7ZQiAQAAAAAojPIcuuPi4iRJO3fuVExMjEqUKGHr8/LyUuXKldWxY8d8LxAAAAAAgMIqz6F73LhxkqTKlSvrH//4h3x8fJxWFAAAAAAARUGeQ3eOHj16OKMOAAAAAACKHIdDd+nSpW84kZrJZJKPj48iIyPVs2dP9erVK18KBAAAAACgsHI4dI8dO1avvfaa2rRpo/vvv1+S9MMPP2jNmjUaMGCADh8+rP79++vKlSvq06dPvhcMAAAAAEBh4XDo/u677/Tqq6+qX79+du3vvPOOvvnmG/3nP//Rvffeqzlz5hC6AQAAAADFmsPrdH/99deKjo7O1d6yZUt9/fXXkqTY2Fj9+uuvd14dAAAAAACFmMOhu0yZMlqxYkWu9hUrVqhMmTKSpAsXLqhkyZJ3Xh0AAAAAAIWYw6eXv/zyy+rfv782bNhgu6Z7+/bt+uqrrzR//nxJ0tq1a9W8efP8rRQAAAAAgELGZBiG4eiDtmzZorfeeksHDhyQJFWrVk2DBg1SkyZN8r3AwsJisSggIEDp6ekym82uLgcAAAAA4ER5zYC3FbqRG6EbAAAAAIqPvGZAh08vl6Ts7GwdOnRIqampys7Otutr1qzZ7WwSAAAAAIAix+HQvXXrVnXt2lVHjx7V9QfJTSaTrFZrvhUHAAAAAEBh5nDo7tevnxo0aKBVq1YpJCREJpPJGXUBAAAAAFDoORy6Dx48qMWLFysyMtIZ9QAAAAAAUGQ4vE53w4YNdejQIWfUAgAAAABAkeLwke5BgwZpxIgRSk5OVu3ateXp6WnXf++99+ZbcQAAAAAAFGYOLxnm5pb74LjJZJJhGMV6IjWWDAMAAACA4sNpS4YdPnz4jgoDAAAAAKC4cDh0V6pUyRl1AAAAAABQ5Dg8kZokffjhh2ratKlCQ0N19OhRSdKsWbO0bNmyfC0OAAAAAIDCzOHQPW/ePA0fPlyxsbFKS0uzXcNdqlQpzZo1K7/rAwAAAACg0HI4dL/55pt699139eKLL8rd3d3W3qBBA/3444/5WhwAAAAAAIWZw6H78OHDqlu3bq52b29vXbhwIV+KAgAAAACgKHA4dIeHh2vnzp252tesWaMaNWrkR00AAAAAABQJDs9ePnz4cA0YMECXLl2SYRj64Ycf9Omnn2ry5Ml67733nFEjAAAAAACFksOhu3fv3vL19dVLL72kjIwMde3aVaGhoZo9e7a6dOnijBoBAAAAACiUTIZhGLf74IyMDJ0/f16BgYHKyMjQzp071aRJk/ysr9CwWCwKCAhQenq6zGazq8sBAAAAADhRXjOgw0e6r+Xn5yc/Pz9J0sGDBxUVFWVbQgwAAAAAgOLO4YnUAAAAAABA3hC6AQAAAABwEkI3AAAAAABOkudrupcvX37L/sOHD99xMQAAAAAAFCV5Dt1xcXF/OsZkMt1JLQAAAAAAFCl5Dt3Z2dnOrAMAAAAAgCKHa7oBAAAAAHASQjcAAAAAAE5C6AYAAAAAwEkI3QAAAAAAOAmhGwAAAAAAJ3E4dFepUkWnT5/O1Z6WlqYqVarkS1EAAAAAABQFDofuI0eOyGq15mrPzMzUiRMn8qUoAAAAAACKgjyv0718+XLb7a+//loBAQG2+1arVevWrVPlypXztTgAAAAAAAqzPIfuuLg4SZLJZFKPHj3s+jw9PVW5cmXNnDkzX4sDAAAAAKAwy3Pozs7OliSFh4dr+/btKleunNOKAgAAAACgKMhz6M5x+PBhZ9QBAAAAAECRc1tLhq1bt07t2rVTRESEIiIi1K5dO3377bf5XRsAAAAAAIWaw6F77ty5at26tUqWLKkhQ4ZoyJAhMpvNio2N1dtvv+2MGgEAAAAAKJRMhmEYjjygQoUKGj16tAYOHGjX/vbbb2vSpEnFdtkwi8WigIAApaeny2w2u7ocAAAAAIAT5TUDOnykOy0tTa1bt87V3qpVK6Wnpzu6OQAAAAAAiiyHQ3f79u315Zdf5mpftmyZ2rVrly9FAQAAAABQFDg8e3nNmjX12muvaePGjWrcuLEkaevWrdqyZYtGjBihOXPm2MYOHjw4/yoFAAAFjtVqVXx8vJKSkhQSEqKoqCi5u7u7uiwAAAoMh6/pDg8Pz9uGTSb9+uuvt1VUYcQ13QCA4mbJkiUaMWKEjhw5YmurXLmyZs6cqQ4dOriuMAAA/gJ5zYCs0w0AABy2ZMkSderUSe3atdOnn36qWrVqac+ePZo0aZI6deqkxYsXE7wBANBtHOnGjXGkGwBQXFitVkVGRqp27dpaunSp3Nz+mCImOztbcXFx2rNnjw4ePMip5gCAIstpR7oNw9DixYu1YcMGpaamKjs7265/yZIljlcLAAAKjfj4eB05ckSffvqpXeCWJDc3N40ZM0ZNmjRRfHy8HnzwQdcUCQBAAeFw6B46dKjeeecdtWjRQkFBQTKZTM6oCwAAFFBJSUmSpFq1at2wP6c9ZxwAAMWZw6H7ww8/1JIlSxQbG+uMegAAQAEXEhIiSdqzZ48aNWqUq3/Pnj124wAAKM4cXqc7ICBAVapUcUYtAACgEIiKilLlypU1adKkXJeZZWdna/LkyQoPD1dUVJSLKgQAoOBwOHSPHz9eEyZM0MWLF51RDwAAKODc3d01c+ZMrVy5UnFxcUpISNC5c+eUkJCguLg4rVy5UjNmzGASNQAAdBuzl1+8eFGPPvqotmzZosqVK8vT09Ouf8eOHflaYGHB7OUAgOLmRut0h4eHa8aMGSwXBgAo8pw2e3mPHj2UmJioJ554gonUAAAoxjp06KC///3vio+PV1JSkkJCQhQVFcURbgAAruHwkW5/f399/fXXeuCBB5xVU6HEkW4AAAAAKD7ymgEdvqY7LCyMUAkAAAAAQB44HLpnzpyp559/3u76LQAAAAAAkJvD13Q/8cQTysjIUEREhPz8/HJNpHbmzJl8Kw4AAAAAgMLM4dA9a9YsJ5QBAAAAAEDRc1uzlwMAAAAAgD/ncOiWJKvVqqVLl2rfvn2SpHvuuUft27dniRAAAAAAAK7hcOg+dOiQYmNjdeLECVWrVk2SNHnyZIWFhWnVqlWKiIjI9yIBAAAAACiMHJ69fPDgwYqIiNDx48e1Y8cO7dixQ8eOHVN4eLgGDx7sjBoBAAAAACiUHD7SvWnTJm3dulVlypSxtZUtW1ZTpkxR06ZN87U4AAAAAAAKM4ePdHt7e+vcuXO52s+fPy8vL698KQoAAAAAgKLA4dDdrl079e3bV9u2bZNhGDIMQ1u3blW/fv3Uvn17Z9QIAAAAAECh5HDonjNnjiIiItS4cWP5+PjIx8dHTZs2VWRkpGbPnu2MGgEAAAAAKJQcuqbbMAxZLBZ99tlnOnHihG3JsBo1aigyMtIpBQIAAAAAUFg5HLojIyO1d+9eVa1alaANAAAAAMAtOBS63dzcVLVqVZ0+fVpVq1Z1Vk2FW/XqkpvDZ+3b++gj6cEH/7i/caP0xBNXbw8ffvUvx7lzUo0ad/Z8ktS8ufTxx/ZtDz0k/fyzVKKEtH+/fd/IkdKnn975865bJ/3/eu+SpE8+kZ5//urtadOkrl3/6DtwQGrZ8s6f8/HHpenT7duqV5fOn5fuvltav96+r1s3adOmO3/effukkiX/uP/661f/pFu/53fiVp+XW73nd+LPPi+3es/vxK0+L7d6z+/En31ebvWe3wm+I67e5jvizp+T74irt/mOuPPn5DuC7wiJ7whH8R1R+L8jsrPzNMzhJcOmTJmikSNHat68eapVq5ajD78jU6ZM0ZgxYzRkyBDNmjVLknTp0iWNGDFCn332mTIzMxUTE6O5c+cqKCjI9rhjx46pf//+2rBhg0qUKKEePXpo8uTJ8vD4Y/c3btyo4cOHa+/evQoLC9NLL72knj17Ol5kUtId7qWkzMzc90+cuHrbYrHvM4w/+u7E77/nbktJubrta/+x5zh7Nn+e98oV+/sZGX9sNyMj99j8eM6zZ3O3nTx59UslICB33++/58/zGob9fYvlj+3e6j2/E7f6vNzqPb8Tf/Z5udV7fidu9Xm51Xt+J/7s83Kr9/xO8B3xx+3rx/Id4Ri+I/54/uvxHeEYviP4jpD4jnAU3xFF8zviBhwO3d27d1dGRobq1KkjLy8v+fr62vWfOXMm34q71vbt2/XOO+/o3nvvtWsfNmyYVq1apUWLFikgIEADBw5Uhw4dtGXLFkmS1WpV27ZtFRwcrO+//15JSUnq3r27PD09NWnSJEnS4cOH1bZtW/Xr108ff/yx1q1bp969eyskJEQxMTGOFRoScudHur29c9+/666rt81m+z6T6Y++O1GuXO62oCApPf3qr0/XK106f57X47qPoJ/fH9v188s9Nj+es3Tp3G2hoVd/rbzmxxqbcuXy53lNJvv7ZvMf273Ve34nbvV5udV7fif+7PNyq/f8Ttzq83Kr9/xO/Nnn5Vbv+Z3gO+KP29eP5TvCMXxH/PH81+M7wjF8R/AdIfEd4Si+Iwr/d0R2dp4OupoM4/qfUG5t4cKFMl3/AbhGjx49HNlcnpw/f1716tXT3Llz9eqrr+q+++7TrFmzlJ6ervLly+uTTz5Rp06dJEn79+9XjRo1lJCQoEaNGmn16tVq166dTp48aTv6PX/+fI0aNUqnTp2Sl5eXRo0apVWrVmnPnj225+zSpYvS0tK0Zs2aPNVosVgUEBCg9PR0ma//sAIAAAAAipS8ZkCHj3Q//vjjunLlivz9/e+oQEcMGDBAbdu2VXR0tF599VVbe2Jioi5fvqzo6GhbW/Xq1VWxYkVb6E5ISFDt2rXtTjePiYlR//79tXfvXtWtW1cJCQl228gZM3ToUKfvGwAAAACg6MrzedCnTp1SmzZtVKJECZnNZjVq1EiHDh1yZm2SpM8++0w7duzQ5MmTc/UlJyfLy8tLpUqVsmsPCgpScnKybUzQdadu5Nz/szEWi0UXL168YV2ZmZmyWCx2fwAAFDdWq1UbN27Up59+qo0bN8pqtbq6JAAACpQ8h+5Ro0Zp586dmjhxombMmKG0tDT16dPHmbXp+PHjGjJkiD7++GP5+Pg49bkcNXnyZAUEBNj+wsLCXF0SAAB/qSVLligyMlItWrRQ165d1aJFC0VGRmrJkiWuLg0AgAIjz6F77dq1WrhwocaMGaNhw4ZpxYoVio+PV+b1s9/lo8TERKWmpqpevXry8PCQh4eHNm3apDlz5sjDw0NBQUHKyspSWlqa3eNSUlIUHBwsSQoODlZKSkqu/py+W40xm825JorLMWbMGKWnp9v+jh8/nh+7DABAobBkyRJ16tRJtWvXVkJCgs6dO2e7pKtTp04EbwAA/l+eQ/fJkydVp04d2/2qVavK29tbSfmxRNZNtGzZUj/++KN27txp+2vQoIG6detmu+3p6al169bZHnPgwAEdO3ZMjRs3liQ1btxYP/74o1JTU21j1q5dK7PZrJo1a9rGXLuNnDE527gRb29vmc1muz8AAIoDq9WqESNGqF27dlq6dKkaNWqkEiVKqFGjRlq6dKnatWun5557jlPNAQCQgxOpubu757rv4OTnDilZsmSutcD9/f1VtmxZW/vTTz+t4cOHq0yZMjKbzRo0aJAaN26sRo0aSZJatWqlmjVr6sknn9S0adOUnJysl156SQMGDJD3/0+X369fP7311lt6/vnn9dRTT2n9+vX64osvtGrVKqftGwAAhVV8fLyOHDmiTz/9VG7XLZPp5uamMWPGqEmTJoqPj9eDDz7omiIBACgg8hy6DcPQ3Xffbbdc2Pnz51W3bl27/+A6a53um3njjTfk5uamjh07KjMzUzExMZo7d66t393dXStXrlT//v3VuHFj+fv7q0ePHpo4caJtTHh4uFatWqVhw4Zp9uzZqlChgt577z3H1+gGAKAYyDnLrVatWrJarYqPj1dSUpJCQkIUFRVl+2HcmWfDAQBQWOQ5dP/rX/9yZh15tnHjRrv7Pj4+evvtt/X222/f9DGVKlXSV199dcvtPvjgg/rf//6XHyUCAFCkhYSESJLeeustvfPOOzpy5Iitr3Llyurbt6/dOAAAijOT4czzw4uRvC6MDgBAYWe1WhUaGqrU1FS1a9dOL774omrVqqU9e/botdde08qVKxUYGKiTJ0/mujQNAICiIq8ZMM8TqQEAAOS49jd7wzBsfwAAwN4dhe62bdtyvRYAAMVMfHy8Tp06pcmTJ2vPnj1q0qSJzGazmjRpor1792rSpElKTU1VfHy8q0sFAMDl7ih0b968WRcvXsyvWgAAQCGQ84P7wIEDdejQIW3YsEGffPKJNmzYoIMHD2rgwIF24wAAKM4cWjIMAAAgZ4K0PXv2qFGjRrmWBduzZ4/dOAAAirM7vqb72iXEAABA0RcVFaXKlStr0qRJys7OtuvLzs7W5MmTFR4erqioKBdVCABAweFQ6HZzc5O7u7vtLyMjQ5GRkXJ3d7f1AQCAos3d3V0zZ87UypUrFRcXp4SEBJ07d04JCQmKi4vTypUrNWPGDP5/AQAAcvD08sOHD9tuG4ahWrVq6auvvlKlSpXyvTAAAFBwdejQQYsXL9aIESPUpEkTW3t4eLgWL16sDh06uLA6AAAKjjtap7tkyZLatWuXqlSpkp81FUqs0w0AKI6sVqvi4+OVlJSkkJAQRUVFcYQbAFAs5DUDMpEaAAC4be7u7rkmUgMAAH+4o4nUKlWqJE9Pz/yqBQAAAACAIuWOjnTnLAkCAAAAAAByu+MlwwAAAAAAwI3dVuj+8MMP1bRpU4WGhuro0aOSpFmzZmnZsmX5WhwAAAAAAIWZw6F73rx5Gj58uGJjY5WWliar1SpJKlWqlGbNmpXf9QEAAAAAUGg5HLrffPNNvfvuu3rxxRftlgRp0KCBfvzxx3wtDgAAAACAwszh0H348GHVrVs3V7u3t7cuXLiQL0UBAAAAAFAUOBy6w8PDtXPnzlzta9asUY0aNfKjJgAAAAAAigSHlwwbPny4BgwYoEuXLskwDP3www/69NNPNXnyZL333nvOqBEAAAAAgELJ4dDdu3dv+fr66qWXXlJGRoa6du2q0NBQzZ49W126dHFGjQAAAAAAFEomwzCM231wRkaGzp8/r8DAwPysqVCyWCwKCAhQenq6zGazq8sBAAAAADhRXjOgw0e6r+Xn5yc/P7872QQAAAAAAEVWnkJ33bp1ZTKZ8rTBHTt23FFBAAAAAAAUFXkK3XFxcbbbly5d0ty5c1WzZk01btxYkrR161bt3btXzz77rFOKBAAAAACgMMpT6B43bpztdu/evTV48GC98sorucYcP348f6sDAAAAAKAQc3gitYCAAP33v/9V1apV7doPHjyoBg0aKD09PV8LLCyYSA0AUBxZrVbFx8crKSlJISEhioqKkru7u6vLAgDA6fKaAd0c3bCvr6+2bNmSq33Lli3y8fFxdHMAAKCQWrJkiSIjI9WiRQt17dpVLVq0UGRkpJYsWeLq0gAAKDAcnr186NCh6t+/v3bs2KH7779fkrRt2zb985//1Msvv5zvBQIAgIJnyZIl6tSpk9q2bauRI0fK19dXFy9e1OrVq9WpUyctXrxYHTp0cHWZAAC43G2t0/3FF19o9uzZ2rdvnySpRo0aGjJkiDp37pzvBRYWnF4OACgurFarIiMjVa5cOf3+++86cuSIra9y5coqV66cTp8+rYMHD3KqOQCgyHLa6eWS1LlzZ23ZskVnzpzRmTNntGXLlmIduAEAKE7i4+N15MgRJSYmqnbt2kpISNC5c+eUkJCg2rVrKzExUYcPH1Z8fLyrSwUAwOUcPr08R2Jiou1I9z333KO6devmW1EAAKDgOnHihCSpdevWWrp0qdzcrv6G36hRIy1dulTt2rXT6tWrbeMAACjOHA7dqamp6tKlizZu3KhSpUpJktLS0tSiRQt99tlnKl++fH7XCAAACpBTp05Jkjp06GAL3Dnc3NwUFxen1atX28YBAFCcOXx6+aBBg3Tu3Dnt3bvXdnr5nj17ZLFYNHjwYGfUCAAACpCcH9iXLFmi7Oxsu77s7GwtXbrUbhwAAMWZw6F7zZo1mjt3rmrUqGFrq1mzpt5++22tXr06X4sDAAAFz1133SVJWr16teLi4uyu6c45yn3tOAAAijOHTy/Pzs6Wp6dnrnZPT89cv3YDAICiJyoqyjZL+e7du9WkSRNbX+XKldWgQQOdPn1aUVFRLqwSAICCweHQ/dBDD2nIkCH69NNPFRoaKunqhCrDhg1Ty5Yt871AAABQsLi7u2vmzJk3XKd7zZo1WrVqlRYvXsxyYQAA6DbW6T5+/Ljat2+vvXv3KiwszNZWq1YtLV++XBUqVHBKoQUd63QDAIqbJUuWaMSIEXbrdIeHh2vGjBnq0KGD6woDAOAvkNcM6HDoliTDMPTtt99q//79kqQaNWooOjr69qstAgjdAIDiyGq1Kj4+XklJSQoJCVFUVBRHuAEAxYJTQzdyI3QDAAAAQPGR1wyY59nL169fr5o1a8piseTqS09P1z333KP4+PjbqxYAAAAAgCIoz6F71qxZ6tOnzw0TfEBAgJ555hm9/vrr+VocAAAAAACFWZ5D965du9S6deub9rdq1UqJiYn5UhQAAAAAAEVBnkN3SkrKDdfnzuHh4aFTp07lS1EAAAAAABQFeQ7dd911l/bs2XPT/t27dyskJCRfigIAAAAAoCjIc+iOjY3Vyy+/rEuXLuXqu3jxosaNG6d27drla3EAAAAAABRmeV4yLCUlRfXq1ZO7u7sGDhyoatWqSZL279+vt99+W1arVTt27FBQUJBTCy6oWDIMAAAAAIqPvGZAj7xuMCgoSN9//7369++vMWPGKCerm0wmxcTE6O233y62gRsAAAAAgBvJc+iWpEqVKumrr77S2bNndejQIRmGoapVq6p06dLOqg8AAAAAgELLodCdo3Tp0vrb3/6W37UAAAAAAFCk5HkiNQAAAAAA4BhCNwAAAAAATkLoBgAAAADASQjdAAAAAAA4CaEbAAAAAAAnIXQDAAAAAOAkhG4AAAAAAJyE0A0AAAAAgJMQugEAAAAAcBJCNwAAAAAATkLoBgAAAADASQjdAAAAAAA4CaEbAAAAAAAnIXQDAAAAAOAkhG4AAAAAAJyE0A0AAAAAgJMQugEAAAAAcBJCNwAAAAAATkLoBgAAAADASQjdAAAAAAA4CaEbAAAAAAAnIXQDAAAAAOAkhG4AAAAAAJzEw9UFAACAwstqtSo+Pl5JSUkKCQlRVFSU3N3dXV0WAAAFBke6AQDAbVmyZIkiIyPVokULde3aVS1atFBkZKSWLFni6tIAACgwCN0AAMBhS5YsUadOnVS7dm0lJCTo3LlzSkhIUO3atdWpUyeCNwAA/89kGIbh6iKKAovFooCAAKWnp8tsNru6HAAAnMZqtSoyMlK1a9fW0qVL5eb2x2/42dnZiouL0549e3Tw4EFONQcAFFl5zYAc6QYAAA6Jj4/XkSNH9MILL9gFbklyc3PTmDFjdPjwYcXHx7uoQgAACg5CNwAAcEhSUpIkqVatWjfsz2nPGQcAQHFG6AYAAA4JCQmRJO3Zs+eG/TntOeMAACjOCN0AAMAhUVFRqly5siZNmqTs7Gy7vuzsbE2ePFnh4eGKiopyUYUAABQchG4AAOAQd3d3zZw5UytXrlRcXJzd7OVxcXFauXKlZsyYwSRqAABI8nB1AQAAoPDp0KGDFi9erBEjRqhJkya29vDwcC1evFgdOnRwYXUAABQcLBmWT1gyDABQHFmtVsXHxyspKUkhISGKioriCDcAoFhgyTAAAAAAAFyM0A0AAG7LkiVLFBkZqRYtWqhr165q0aKFIiMjtWTJEleXBgBAgUHoBgAADluyZIk6deqklJQUu/aUlBR16tSJ4A0AwP8jdAMAAIdYrVb1799fhmHo+qlhctr69+8vq9XqogoBACg4CN0AAMAhGzduVGpqqiQpOjrabsmw6OhoSVJqaqo2btzowioBACgYCN0AAMAh69evlyQ1btxYixYt0tatWzVmzBht3bpVixYtUsOGDe3GAQBQnLFONwAAcMixY8ckSX5+fipRooTdaeTPPfecmjdvbjcOAIDijNANAAAcUrFiRUnSunXrcvVZrVbbEe6ccQAAFGecXg4AABzywAMP2G57eXlp9OjROnjwoEaPHi0vL68bjgMAoLgidAMAAIfs3r3bdtvd3V1TpkxR1apVNWXKFLm7u99wHAAAxRWhGwAAOGTFihW229cvGXazcQAAFFcFOnRPnjxZf/vb31SyZEkFBgYqLi5OBw4csBtz6dIlDRgwQGXLllWJEiXUsWNHpaSk2I05duyY2rZtKz8/PwUGBmrkyJG6cuWK3ZiNGzeqXr168vb2VmRkpBYuXOjs3QMAoFDr3r27AgMD7doCAwP1xBNPuKgiAAAKngIdujdt2qQBAwZo69atWrt2rS5fvqxWrVrpwoULtjHDhg3TihUrtGjRIm3atEknT55Uhw4dbP1Wq1Vt27ZVVlaWvv/+e33wwQdauHChxo4daxtz+PBhtW3bVi1atNDOnTs1dOhQ9e7dW19//fVfur8AABQGf//73yVJixYtkslkytW/ePFiu3EAABRnJuNW54UVMKdOnVJgYKA2bdqkZs2aKT09XeXLl9cnn3yiTp06SZL279+vGjVqKCEhQY0aNdLq1avVrl07nTx5UkFBQZKk+fPna9SoUTp16pS8vLw0atQorVq1Snv27LE9V5cuXZSWlqY1a9bkqTaLxaKAgAClp6fLbDbn/84DAFBAZGVlydvbW9LVidSGDRump59+Wu+//77eeOMNZWVlSZIyMzPtJlYDAKAoyWsGLNBHuq+Xnp4uSSpTpowkKTExUZcvX1Z0dLRtTPXq1VWxYkUlJCRIkhISElS7dm1b4JakmJgYWSwW7d271zbm2m3kjMnZxo1kZmbKYrHY/QEAUBy4u7srICBA0tUAPnXqVN19992aOnWqLXAHBATYTaoGAEBxVWhCd3Z2toYOHaqmTZuqVq1akqTk5GR5eXmpVKlSdmODgoKUnJxsG3Nt4M7pz+m71RiLxaKLFy/esJ7JkycrICDA9hcWFnbH+wgAQGEQHx+v9PR0devWLVew9vDwUNeuXZWenq74+HgXVQgAQMFRaEL3gAEDtGfPHn322WeuLkWSNGbMGKWnp9v+jh8/7uqSAAD4SyQlJUm6ernWuXPnNGDAALVq1UoDBgyQxWLR/Pnz7cYBAFCcebi6gLwYOHCgVq5cqc2bN6tChQq29uDgYGVlZSktLc3uaHdKSoqCg4NtY3744Qe77eXMbn7tmOtnPE9JSZHZbJavr+8Na/L29rZdzwYAQHESEhIiSXrrrbf0zjvv6MiRI5Kkb775RqtWrVLfvn3txgEAUJwV6CPdhmFo4MCB+vLLL7V+/XqFh4fb9devX1+enp5at26dre3AgQM6duyYGjduLElq3LixfvzxR6WmptrGrF27VmazWTVr1rSNuXYbOWNytgEAAP4QFRWlwMBAjRkzRrVq1VJCQoLOnTunhIQE1apVSy+88IICAwMVFRXl6lIBAHC5An2ke8CAAfrkk0+0bNkylSxZ0nYNdkBAgHx9fRUQEKCnn35aw4cPV5kyZWQ2mzVo0CA1btxYjRo1kiS1atVKNWvW1JNPPqlp06YpOTlZL730kgYMGGA7Ut2vXz+99dZbev755/XUU09p/fr1+uKLL7Rq1SqX7TsAAAXZtYufGIZh+wMAAPYK9JHuefPmKT09XQ8++KBCQkJsf59//rltzBtvvKF27dqpY8eOatasmYKDg7VkyRJbv7u7u1auXCl3d3c1btxYTzzxhLp3766JEyfaxoSHh2vVqlVau3at6tSpo5kzZ+q9995TTEzMX7q/AAAUBvHx8Tp16pQmT56sPXv2qEmTJjKbzWrSpIn27t2rSZMmKTU1lYnUAABQIVunuyBjnW4AQHHx6aefqmvXrjp37py8vLw0d+5c/fLLL4qIiNCzzz6rzMxMmc1mffLJJ3r88cddXS4AAE6R1wxYoE8vBwAABc/NJlKTpNmzZzORGgAA1yjQp5cDAICCh4nUAADIO0I3AABwGBOpAQCQN4RuAADgkGsnUvvxxx/tJlLbs2cPE6kBAHANQjcAAHBIUlKSJCksLEwmkylXf8WKFe3GAQBQnDGRGgAAcEjOBGlPPvmkYmNj9fe//10XL16Ur6+vDh06pCeffNJuHAAAxRlLhuUTlgwDABQXWVlZ8vf3l5eXlzIzM2W1Wm197u7u8vb2VlZWli5cuCAvLy8XVgoAgPPkNQNyejkAAHDI999/rytXrigjI0Pu7u4aPXq0Dh48qNGjR8vd3V0ZGRm6cuWKvv/+e1eXCgCAy3F6OQAAcMjx48clSWazWQEBAZoyZYqmTJki6er13GlpabJYLLZxAAAUZxzpBgAADtm2bZskqWXLlnJzs/+/EiaTSQ899JDdOAAAijOOdAMAAIfkTAfz5Zdf5uo7evSojh49ajcOAIDijCPdAADAIREREfk6DgCAoozQDQAAHFKjRo18HQcAQFFG6AYAAA75+OOP83UcAABFGaEbAAA45PDhw/k6DgCAooyJ1AAAgEMyMjJst8uWLauWLVvK399fFy5c0Lp163T69Olc4wAAKK4I3QAAwCFZWVm2240aNdLQoUNVq1Yt7dmzRxcuXNCqVatyjQMAoLgidAMAAIdcewR79erVtpAtyW7dbo50AwDANd0AAMBBISEhttvZ2dl2fdfev3YcAADFFaEbAAA4pEOHDvk6DgCAosxkGIbh6iKKAovFooCAAKWnp8tsNru6HAAAnCYrK0ve3t5/Oi4zM1NeXl5/QUUAAPz18poBOdINAAAc4u7uLj8/v1uO8fPzk7u7+19UEQAABRehGwAAOGTjxo1/OklaRkaGNm7c+NcUBABAAUboBgAADlm/fr2kqxOlXX80293d3TaBWs44AACKM0I3AABwyPHjxyVJSUlJun5qGMMwlJSUZDcOAIDijNANAAAcwpJhAADkHaEbAAA4JD093Xbb09NTo0eP1sGDBzV69Gh5enrecBwAAMWVh6sLAAAAhcuJEydstz08PDRlyhRNmTJFkuTr66vLly/nGgcAQHHFkW4AAOCQkydP2m7f6JruG40DAKC4InQDAACHVKhQQdLVmcovXbpk13fp0iXbjOY54wAAKM4I3QAAwCHNmjWTJFmt1hv257TnjAMAoDgzGdefF4bbYrFYFBAQoPT0dJnNZleXAwCA06Snp6tUqVJ/Oi4tLU0BAQHOLwgAABfIawbkSDcAAHDI888/n6/jAAAoygjdAADAIatWrcrXcQAAFGWEbgAA4JBz587l6zgAAIoyQjcAAHBIyZIl83UcAABFGaEbAAA4JCMjI1/HAQBQlBG6AQCAQwjdAADkHaEbAAA45MqVK/k6DgCAoozQDQAAHOLl5ZWv4wAAKMoI3QAAwCHBwcH5Og4AgKKM0A0AABzC7OUAAOQdoRsAADjk9OnT+ToOAICijNANAAAc4uaWt//7kNdxAAAUZfzXEAAAOMRiseTrOAAAijJCNwAAcEh6enq+jgMAoCgjdAMAAAAA4CSEbgAAAAAAnITQDQAAHGIymfJ1HAAARRmhGwAAOMQwjHwdBwBAUUboBgAAAADASQjdAAAAAAA4CaEbAAAAAAAnIXQDAAAAAOAkhG4AAAAAAJyE0A0AAAAAgJMQugEAAAAAcBJCNwAAAAAATkLoBgAAAADASQjdAAAAAAA4CaEbAAAAAAAnIXQDAAAAAOAkhG4AAAAAAJyE0A0AAAAAgJMQugEAAAAAcBJCNwAAAAAATkLoBgAAAADASTxcXQAAALg9GRkZ2r9/v6vLuKUdO3b8pc9XvXp1+fn5/aXPCQDArRC6AQAopPbv36/69eu7uoxb+qvrS0xMVL169f7S5wQA4FYI3QAAFFLVq1dXYmLiX/68jgTpv7q+6tWr/6XPBwDAnyF0AwBQSPn5+bnkqO7Ro0dVqVKlPI2rWLHiX1ARAAAFFxOpAQAAh1SsWFEeHrf+3d7Dw4PADQCACN0AAOA2XL58+abB28PDQ5cvX/6LKwIAoGAidAMAgNty+fJlHT161DZbuJ+fn44ePUrgBgDgGoRuAABw2ypWrKj4+HhJUnx8PKeUAwBwHUI3AAAAAABOQugGAAAAAMBJCN0AAAAAADgJoRsAAAAAACe59SKbAADghk6dOiWLxeLqMgqE48eP2/43ICDAxdW4ntlsVvny5V1dBgCggCB0AwDgoFOnTqn3M8/owsVLri6lQLCkp0uSXps6TWZCt/x9ffTeO+8QvAEAkgjdAAA4zGKx6MLFS4p7qp8CQ+5ydTkul3npkk4cPaK7KlWWt4+Pq8txqdSkE1r6z/myWCyEbgCAJEI3AAC3LTDkLlWoHO7qMgqEiOo1XF0CAAAFEhOpAQAAAADgJIRuAAAAAACchNPLAQC4DWU8s1TK8rP8f7/g6lJQgJSynFAZzyxXlwEAKEAI3QAA3IZ2gclq9UM/V5eBAig5MMzVJQAAChBCNwAAt2FlarCCO7yooFBmL8cfUk6e0Mod76upqwsBABQYhG4AAG7Dmcte+vmCv9LO+7u6FJdjybA/pF7w15nLXq4uAwBQgBC6AQBwkNlslr+vj5b+c76rSykQLOnp2p7wvf7WuInMAQGuLsfl/H19ZDabXV0GAKCAMBmGYbi6iKLAYrEoICBA6enp/IcWAIqBU6dOyWKxuLqMAmHPnj2Ki4vT0qVLVatWLVeX43Jms1nly5d3dRkAACfLawbkSDcAALehfPnyBKv/l56eLkkKCwtTRESEi6sBAKBgIXQDAFBIZWRkaP/+/a4uQ/v27bP7X1eqXr26/Pz8XF0GAAA2hG4AAAqp/fv3q379+q4uw+aJJ55wdQlKTExUvXr1XF0GAAA2hO7rvP3225o+fbqSk5NVp04dvfnmm7r//vtdXRYAALlUr15diYmJri5DFy9e1JEjR1S5cmX5+vq6tJbq1au79PkBALgeE6ld4/PPP1f37t01f/58NWzYULNmzdKiRYt04MABBQYG3vKxTKQGAAAAAMVHXjOg219YU4H3+uuvq0+fPurVq5dq1qyp+fPny8/PT//85z9dXRoAAAAAoBDi9PL/l5WVpcTERI0ZM8bW5ubmpujoaCUkJOQan5mZqczMTNv9nJlbWT4GAAAAAIq+nOz3ZyePE7r/3++//y6r1aqgoCC79qCgoBvODDt58mRNmDAhV3tYWJjTagQAAAAAFCznzp1TQEDATfsJ3bdpzJgxGj58uO1+dna2zpw5o7Jly8pkMrmwMgAA/loWi0VhYWE6fvw485oAAIoNwzB07tw5hYaG3nIcofv/lStXTu7u7kpJSbFrT0lJUXBwcK7x3t7e8vb2tmsrVaqUM0sEAKBAM5vNhG4AQLFyqyPcOZhI7f95eXmpfv36Wrduna0tOztb69atU+PGjV1YGQAAAACgsOJI9zWGDx+uHj16qEGDBrr//vs1a9YsXbhwQb169XJ1aQAAAACAQojQfY1//OMfOnXqlMaOHavk5GTdd999WrNmTa7J1QAAwB+8vb01bty4XJddAQAAyWT82fzmAAAAAADgtnBNNwAAAAAATkLoBgAAAADASQjdAAAAAAA4CaEbAAAAAAAnIXQDAIDbsnnzZj3yyCMKDQ2VyWTS0qVLXV0SAAAFDqEbAADclgsXLqhOnTp6++23XV0KAAAFFut0AwCA29KmTRu1adPG1WUAAFCgcaQbAAAAAAAnIXQDAAAAAOAkhG4AAAAAAJyE0A0AAAAAgJMQugEAAAAAcBJmLwcAALfl/PnzOnTokO3+4cOHtXPnTpUpU0YVK1Z0YWUAABQcJsMwDFcXAQAACp+NGzeqRYsWudp79OihhQsX/vUFAQBQABG6AQAAAABwEq7pBgAAAADASQjdAAAAAAA4CaEbAAAAAAAnIXQDAAAAAOAkhG4AAAAAAJyE0A0AAAAAgJMQugEAAAAAcBJCNwAAAAAATkLoBgAAAADASQjdAAAAAAA4CaEbAAAAAAAnIXQDAAAAAOAk/wfChLUWLtaehAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 7,583\n",
      "Samples above 4096 tokens: 3\n",
      "Percentage above 4096: 0.04%\n",
      "Max length: 6,479 tokens\n"
     ]
    }
   ],
   "source": [
    "# Simple analysis of sample lengths around 4096 characters\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Calculate statistics\n",
    "samples_above_4096 = sum(1 for s in sample_lengths.values() if s > 4096)\n",
    "samples_3000_6000 = [s for s in sample_lengths if 3000 <= s <= 6000]\n",
    "\n",
    "# Single focused plot\n",
    "plt.boxplot(sample_lengths.values(), vert=True, patch_artist=True, \n",
    "           boxprops=dict(facecolor='lightblue', alpha=0.7))\n",
    "plt.ylabel('Code+Prompt Length (tokens)')\n",
    "plt.title('Code+Prompt Sample Lengths Distribution')\n",
    "plt.ylim(0, 10000)\n",
    "\n",
    "# Add 4096 reference line\n",
    "plt.axhline(4096, color='red', linestyle='-.', linewidth=2, label='4096 tokens')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"Total samples: {len(sample_lengths.values()):,}\")\n",
    "print(f\"Samples above 4096 tokens: {samples_above_4096}\")\n",
    "print(f\"Percentage above 4096: {samples_above_4096/len(sample_lengths.values())*100:.2f}%\")\n",
    "print(f\"Max length: {max(sample_lengths.values()):,} tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "153fbecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample length nearest to 4096: 3913\n",
      "Distance from 4096: 183\n",
      "Sample index with nearest length: 6589\n"
     ]
    }
   ],
   "source": [
    "target = 4096\n",
    "nearest_length = min(sample_lengths.values(), key=lambda x: abs(x - target))\n",
    "nearest_distance = abs(nearest_length - target)\n",
    "\n",
    "print(f\"Sample length nearest to 4096: {nearest_length}\")\n",
    "print(f\"Distance from 4096: {nearest_distance}\")\n",
    "\n",
    "nearest_index = [idx for idx, length in sample_lengths.items() if length == nearest_length][0]\n",
    "print(f\"Sample index with nearest length: {nearest_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51d8e390",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda example, idx: sample_lengths.get(idx, 0) <= 4096, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d91c94a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7580"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39ed0254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantization config ready for 4-bit loading\n"
     ]
    }
   ],
   "source": [
    "# Configure quantization for memory efficiency\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(\"Quantization config ready for 4-bit loading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c15bcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/gemma-2b-it...\n",
      "This may take a few minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-25 20:23:32,638 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5fcfcd2cecd45e2b48b5ed42960348b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded successfully!\n",
      "Model device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): GELUActivation()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "    (rotary_emb): GemmaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"google/gemma-2b-it\"  \n",
    "print(f\"Loading {model_name}...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Load model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    #quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.eval()\n",
    "    \n",
    "print(\"✅ Model loaded successfully!\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c84488c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24a9bc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing local model with random sample 3015\n",
      "Target: 0\n",
      "Original code length: 294 characters\n",
      "================================================================================\n",
      "✅ Local Model Test Successful!\n",
      "Modified code length: 232 characters\n",
      "Length change: -62\n",
      "Levenshtein distance: 136\n",
      "Distance/Length ratio: 0.463\n",
      "\n",
      "Original code:\n",
      "----------------------------------------\n",
      "import sys\n",
      "input = sys.stdin.readline\n",
      "n=int(input())\n",
      "a=list(map(int,input().split()))\n",
      "cnt2=0\n",
      "cnt4=0\n",
      "for i in range(n):\n",
      "    if a[i]%4==0:\n",
      "        cnt4+=1\n",
      "    elif a[i]%2==0:\n",
      "        cnt2+=1\n",
      "if cnt2>1:\n",
      "    ans=len(a)-cnt4*3-cnt2\n",
      "if cnt2==1:\n",
      "    ans=len(a)-cnt4*3\n",
      "print(\"Yes\" if ans<=0 else \"No\")\n",
      "\n",
      "\n",
      "Modified code:\n",
      "----------------------------------------\n",
      "import sys\n",
      "\n",
      "n = int(input())\n",
      "a = list(map(int, input().split()))\n",
      "\n",
      "if sum(x % 4 for x in a) > 1:\n",
      "    ans = len(a) - 3 * cnt4 - cnt2\n",
      "elif cnt2 == 1:\n",
      "    ans = len(a) - 3 * cnt4\n",
      "else:\n",
      "    ans = len(a)\n",
      "print(\"Yes\" if ans <= 0 else \"No\")\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test local model with one random sample\n",
    "import random\n",
    "import re\n",
    "\n",
    "def clean_response(response):\n",
    "    pattern = r'```python\\n(.*?)\\n```'\n",
    "    match = re.search(pattern, response, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return \"\"  \n",
    "\n",
    "random_index = random.randint(0, len(dataset))\n",
    "test_sample = dataset[random_index]\n",
    "\n",
    "print(f\"Testing local model with random sample {random_index}\")\n",
    "print(f\"Target: {test_sample['target']}\")\n",
    "print(f\"Original code length: {len(test_sample['code'])} characters\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    # Prepare prompt for Gemma\n",
    "    prompt = f'''<start_of_turn>user\n",
    "Refine this Python code to be more concise and readable while preserving its exact functionality. The code’s input and output behavior must remain unchanged, regardless of input types or structure. Ensure the refined code:\n",
    "- Is valid Python with no syntax errors.\n",
    "- Maintains the same logic and results as the original.\n",
    "- Avoids introducing new dependencies or invalid assumptions (e.g., treating scalars as iterables).\n",
    "Output ONLY the refined Python code. Do NOT include explanations, comments, or any text other than the code itself:\n",
    "\n",
    "{test_sample['code']}\n",
    "<end_of_turn>\n",
    "<start_of_turn>model'''\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=2048,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_length = True,\n",
    "    ).to(device)\n",
    "    length = inputs['length'].item()\n",
    "    del inputs['length']\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=2*length+100,  \n",
    "            min_new_tokens=50,\n",
    "            temperature=0.7,     \n",
    "            top_p=0.9,           \n",
    "            do_sample=True,        \n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode only new tokens\n",
    "    input_length = inputs.input_ids.shape[1]\n",
    "    generated_tokens = outputs[0][input_length:]\n",
    "    response = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "    \n",
    "    # Clean response\n",
    "    # response = response.split('<end_of_turn>')[0].strip()\n",
    "    \n",
    "    # response = response.strip('```').lstrip('```python').strip()\n",
    "    response = clean_response(response)\n",
    "    # Calculate distance\n",
    "    distance = Levenshtein_distance(response, test_sample['code'])\n",
    "    \n",
    "    print(f\"✅ Local Model Test Successful!\")\n",
    "    print(f\"Modified code length: {len(response)} characters\")\n",
    "    print(f\"Length change: {len(response) - len(test_sample['code']):+d}\")\n",
    "    print(f\"Levenshtein distance: {distance}\")\n",
    "    print(f\"Distance/Length ratio: {distance/len(test_sample['code']):.3f}\")\n",
    "    \n",
    "    print(f\"\\nOriginal code:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(test_sample['code'])\n",
    "    \n",
    "    print(f\"\\nModified code:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(response)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Local Model Test Failed: {str(e)}\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ce24056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(path, data):\n",
    "    try:\n",
    "        with open(path, \"w\") as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving results: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "043ec040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing entire dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:   1%|          | 40/7580 [01:24<3:59:41,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 40/7580 samples. Avg. Levenshtein: 300.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:   1%|          | 80/7580 [02:08<2:39:26,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 80/7580 samples. Avg. Levenshtein: 252.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:   2%|▏         | 120/7580 [02:49<2:12:19,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 120/7580 samples. Avg. Levenshtein: 231.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:   2%|▏         | 160/7580 [03:31<2:08:16,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 160/7580 samples. Avg. Levenshtein: 203.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:   3%|▎         | 200/7580 [04:31<2:45:24,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 200/7580 samples. Avg. Levenshtein: 192.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:   3%|▎         | 240/7580 [05:17<2:19:38,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 240/7580 samples. Avg. Levenshtein: 184.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:   4%|▎         | 280/7580 [06:04<2:37:49,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 280/7580 samples. Avg. Levenshtein: 185.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:   4%|▍         | 320/7580 [07:04<2:48:26,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 320/7580 samples. Avg. Levenshtein: 188.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:   5%|▍         | 360/7580 [07:59<2:35:28,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 360/7580 samples. Avg. Levenshtein: 189.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:   5%|▌         | 400/7580 [09:52<3:58:42,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 400/7580 samples. Avg. Levenshtein: 188.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:   6%|▌         | 440/7580 [10:45<2:38:21,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 440/7580 samples. Avg. Levenshtein: 187.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:   6%|▋         | 480/7580 [11:44<3:07:22,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 480/7580 samples. Avg. Levenshtein: 186.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:   7%|▋         | 520/7580 [12:45<2:59:52,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 520/7580 samples. Avg. Levenshtein: 186.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:   7%|▋         | 560/7580 [13:49<3:05:15,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 560/7580 samples. Avg. Levenshtein: 184.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:   8%|▊         | 600/7580 [14:33<2:06:05,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 600/7580 samples. Avg. Levenshtein: 185.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:   8%|▊         | 640/7580 [16:10<4:07:24,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 640/7580 samples. Avg. Levenshtein: 185.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:   9%|▉         | 680/7580 [16:48<2:03:24,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 680/7580 samples. Avg. Levenshtein: 182.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:   9%|▉         | 720/7580 [18:06<4:28:27,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 720/7580 samples. Avg. Levenshtein: 179.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  10%|█         | 760/7580 [20:21<4:05:13,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 760/7580 samples. Avg. Levenshtein: 183.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  11%|█         | 800/7580 [21:07<2:22:36,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 800/7580 samples. Avg. Levenshtein: 182.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  11%|█         | 840/7580 [22:18<3:26:53,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 840/7580 samples. Avg. Levenshtein: 183.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  12%|█▏        | 880/7580 [23:02<2:17:55,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 880/7580 samples. Avg. Levenshtein: 181.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  12%|█▏        | 920/7580 [24:27<2:58:36,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 920/7580 samples. Avg. Levenshtein: 183.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  13%|█▎        | 960/7580 [25:18<2:18:25,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 960/7580 samples. Avg. Levenshtein: 185.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  13%|█▎        | 1000/7580 [26:15<2:33:47,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000/7580 samples. Avg. Levenshtein: 186.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  14%|█▎        | 1040/7580 [27:03<2:09:17,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1040/7580 samples. Avg. Levenshtein: 183.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  14%|█▍        | 1080/7580 [27:53<2:18:22,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1080/7580 samples. Avg. Levenshtein: 182.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  15%|█▍        | 1120/7580 [28:50<2:23:44,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1120/7580 samples. Avg. Levenshtein: 182.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  15%|█▌        | 1160/7580 [30:06<2:48:43,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1160/7580 samples. Avg. Levenshtein: 184.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  16%|█▌        | 1200/7580 [31:10<2:50:24,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1200/7580 samples. Avg. Levenshtein: 185.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  16%|█▋        | 1240/7580 [31:47<1:51:19,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1240/7580 samples. Avg. Levenshtein: 184.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  17%|█▋        | 1280/7580 [32:41<2:05:45,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1280/7580 samples. Avg. Levenshtein: 183.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  17%|█▋        | 1320/7580 [34:04<3:34:03,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1320/7580 samples. Avg. Levenshtein: 184.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  18%|█▊        | 1360/7580 [34:54<2:18:38,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1360/7580 samples. Avg. Levenshtein: 184.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  18%|█▊        | 1400/7580 [36:09<2:40:23,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1400/7580 samples. Avg. Levenshtein: 186.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  19%|█▉        | 1440/7580 [37:06<2:33:50,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1440/7580 samples. Avg. Levenshtein: 186.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  20%|█▉        | 1480/7580 [39:29<8:37:50,  5.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1480/7580 samples. Avg. Levenshtein: 186.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  20%|██        | 1520/7580 [40:45<4:23:25,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1520/7580 samples. Avg. Levenshtein: 191.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  21%|██        | 1560/7580 [41:43<3:01:37,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1560/7580 samples. Avg. Levenshtein: 192.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  21%|██        | 1600/7580 [42:36<2:27:58,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1600/7580 samples. Avg. Levenshtein: 191.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  22%|██▏       | 1640/7580 [43:50<3:01:05,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1640/7580 samples. Avg. Levenshtein: 192.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  22%|██▏       | 1680/7580 [44:30<1:48:32,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1680/7580 samples. Avg. Levenshtein: 191.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  23%|██▎       | 1720/7580 [45:15<1:44:55,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1720/7580 samples. Avg. Levenshtein: 189.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  23%|██▎       | 1760/7580 [46:09<2:24:40,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1760/7580 samples. Avg. Levenshtein: 190.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  24%|██▎       | 1800/7580 [47:15<2:47:30,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1800/7580 samples. Avg. Levenshtein: 190.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  24%|██▍       | 1840/7580 [47:54<1:54:19,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1840/7580 samples. Avg. Levenshtein: 189.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  25%|██▍       | 1880/7580 [48:55<1:58:27,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1880/7580 samples. Avg. Levenshtein: 191.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  25%|██▌       | 1920/7580 [49:42<1:50:19,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1920/7580 samples. Avg. Levenshtein: 192.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  26%|██▌       | 1960/7580 [51:52<4:21:00,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1960/7580 samples. Avg. Levenshtein: 193.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  26%|██▋       | 2000/7580 [52:35<2:00:55,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2000/7580 samples. Avg. Levenshtein: 192.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  27%|██▋       | 2040/7580 [53:38<2:09:01,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2040/7580 samples. Avg. Levenshtein: 195.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  27%|██▋       | 2080/7580 [54:31<1:50:10,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2080/7580 samples. Avg. Levenshtein: 195.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  28%|██▊       | 2120/7580 [55:41<2:28:35,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2120/7580 samples. Avg. Levenshtein: 195.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  28%|██▊       | 2160/7580 [56:35<2:09:33,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2160/7580 samples. Avg. Levenshtein: 195.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  29%|██▉       | 2200/7580 [57:39<2:08:40,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2200/7580 samples. Avg. Levenshtein: 196.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  30%|██▉       | 2240/7580 [58:57<3:01:06,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2240/7580 samples. Avg. Levenshtein: 196.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  30%|███       | 2280/7580 [1:00:04<2:14:16,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2280/7580 samples. Avg. Levenshtein: 197.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  31%|███       | 2320/7580 [1:00:42<1:27:58,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2320/7580 samples. Avg. Levenshtein: 196.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  31%|███       | 2360/7580 [1:01:14<1:12:24,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2360/7580 samples. Avg. Levenshtein: 195.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  32%|███▏      | 2400/7580 [1:02:34<2:08:22,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2400/7580 samples. Avg. Levenshtein: 196.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  32%|███▏      | 2440/7580 [1:04:12<3:43:56,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2440/7580 samples. Avg. Levenshtein: 196.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  33%|███▎      | 2480/7580 [1:05:06<2:18:04,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2480/7580 samples. Avg. Levenshtein: 196.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  33%|███▎      | 2520/7580 [1:06:29<2:37:48,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2520/7580 samples. Avg. Levenshtein: 198.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  34%|███▍      | 2560/7580 [1:07:30<2:16:13,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2560/7580 samples. Avg. Levenshtein: 198.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  34%|███▍      | 2600/7580 [1:08:12<1:30:44,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2600/7580 samples. Avg. Levenshtein: 197.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  35%|███▍      | 2640/7580 [1:09:11<1:41:04,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2640/7580 samples. Avg. Levenshtein: 196.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  35%|███▌      | 2680/7580 [1:09:50<1:16:06,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2680/7580 samples. Avg. Levenshtein: 195.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  36%|███▌      | 2720/7580 [1:10:50<1:46:27,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2720/7580 samples. Avg. Levenshtein: 194.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  36%|███▋      | 2760/7580 [1:11:47<1:51:36,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2760/7580 samples. Avg. Levenshtein: 194.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  37%|███▋      | 2800/7580 [1:12:50<2:25:11,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2800/7580 samples. Avg. Levenshtein: 195.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  37%|███▋      | 2840/7580 [1:13:33<1:42:46,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2840/7580 samples. Avg. Levenshtein: 194.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  38%|███▊      | 2880/7580 [1:14:23<1:32:30,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2880/7580 samples. Avg. Levenshtein: 194.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  39%|███▊      | 2920/7580 [1:15:10<1:31:09,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2920/7580 samples. Avg. Levenshtein: 193.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  39%|███▉      | 2960/7580 [1:15:51<1:13:40,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2960/7580 samples. Avg. Levenshtein: 193.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  40%|███▉      | 3000/7580 [1:16:35<1:21:06,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3000/7580 samples. Avg. Levenshtein: 193.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  40%|████      | 3040/7580 [1:17:39<1:47:16,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3040/7580 samples. Avg. Levenshtein: 193.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  41%|████      | 3080/7580 [1:18:45<2:00:26,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3080/7580 samples. Avg. Levenshtein: 194.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  41%|████      | 3120/7580 [1:19:55<2:17:07,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3120/7580 samples. Avg. Levenshtein: 194.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  42%|████▏     | 3160/7580 [1:20:54<1:56:23,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3160/7580 samples. Avg. Levenshtein: 194.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  42%|████▏     | 3200/7580 [1:22:06<2:10:41,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3200/7580 samples. Avg. Levenshtein: 195.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  43%|████▎     | 3240/7580 [1:22:51<1:32:08,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3240/7580 samples. Avg. Levenshtein: 195.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  43%|████▎     | 3280/7580 [1:24:36<3:40:59,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3280/7580 samples. Avg. Levenshtein: 198.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  44%|████▍     | 3320/7580 [1:25:11<1:28:03,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3320/7580 samples. Avg. Levenshtein: 197.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  44%|████▍     | 3360/7580 [1:26:10<1:35:58,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3360/7580 samples. Avg. Levenshtein: 197.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  45%|████▍     | 3400/7580 [1:26:59<1:12:02,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3400/7580 samples. Avg. Levenshtein: 196.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  45%|████▌     | 3440/7580 [1:27:50<1:13:35,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3440/7580 samples. Avg. Levenshtein: 195.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  46%|████▌     | 3480/7580 [1:28:43<1:32:17,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3480/7580 samples. Avg. Levenshtein: 195.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  46%|████▋     | 3520/7580 [1:29:36<1:27:14,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3520/7580 samples. Avg. Levenshtein: 196.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  47%|████▋     | 3560/7580 [1:30:38<1:33:14,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3560/7580 samples. Avg. Levenshtein: 196.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  47%|████▋     | 3600/7580 [1:31:33<1:37:06,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3600/7580 samples. Avg. Levenshtein: 196.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  48%|████▊     | 3640/7580 [1:32:33<1:27:41,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3640/7580 samples. Avg. Levenshtein: 195.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  49%|████▊     | 3680/7580 [1:33:21<1:17:19,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3680/7580 samples. Avg. Levenshtein: 195.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  49%|████▉     | 3720/7580 [1:34:07<1:11:48,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3720/7580 samples. Avg. Levenshtein: 195.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  50%|████▉     | 3760/7580 [1:34:53<1:13:41,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3760/7580 samples. Avg. Levenshtein: 195.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  50%|█████     | 3800/7580 [1:36:00<1:43:24,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3800/7580 samples. Avg. Levenshtein: 195.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  51%|█████     | 3840/7580 [1:36:50<1:28:19,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3840/7580 samples. Avg. Levenshtein: 194.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  51%|█████     | 3880/7580 [1:37:35<1:15:47,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3880/7580 samples. Avg. Levenshtein: 194.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  52%|█████▏    | 3920/7580 [1:38:46<1:55:22,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3920/7580 samples. Avg. Levenshtein: 194.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  52%|█████▏    | 3960/7580 [1:39:50<1:33:53,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3960/7580 samples. Avg. Levenshtein: 194.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  53%|█████▎    | 4000/7580 [1:40:59<1:36:48,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4000/7580 samples. Avg. Levenshtein: 194.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  53%|█████▎    | 4040/7580 [1:42:33<2:59:47,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4040/7580 samples. Avg. Levenshtein: 194.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  54%|█████▍    | 4080/7580 [1:43:30<1:38:54,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4080/7580 samples. Avg. Levenshtein: 194.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  54%|█████▍    | 4120/7580 [1:44:22<1:15:34,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4120/7580 samples. Avg. Levenshtein: 193.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  55%|█████▍    | 4160/7580 [1:45:11<1:12:08,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4160/7580 samples. Avg. Levenshtein: 193.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  55%|█████▌    | 4200/7580 [1:45:47<57:01,  1.01s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4200/7580 samples. Avg. Levenshtein: 193.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  56%|█████▌    | 4240/7580 [1:46:55<1:40:16,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4240/7580 samples. Avg. Levenshtein: 193.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  56%|█████▋    | 4280/7580 [1:47:41<1:05:55,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4280/7580 samples. Avg. Levenshtein: 193.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  57%|█████▋    | 4320/7580 [1:49:15<1:40:43,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4320/7580 samples. Avg. Levenshtein: 193.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  58%|█████▊    | 4360/7580 [1:50:28<1:24:40,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4360/7580 samples. Avg. Levenshtein: 194.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  58%|█████▊    | 4400/7580 [1:52:49<3:12:05,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4400/7580 samples. Avg. Levenshtein: 194.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  59%|█████▊    | 4440/7580 [1:53:44<1:28:03,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4440/7580 samples. Avg. Levenshtein: 193.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  59%|█████▉    | 4480/7580 [1:54:41<1:20:23,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4480/7580 samples. Avg. Levenshtein: 193.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  60%|█████▉    | 4520/7580 [1:55:33<1:06:45,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4520/7580 samples. Avg. Levenshtein: 193.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  60%|██████    | 4560/7580 [1:56:17<55:37,  1.11s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4560/7580 samples. Avg. Levenshtein: 193.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  61%|██████    | 4600/7580 [1:57:23<1:10:55,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4600/7580 samples. Avg. Levenshtein: 193.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  61%|██████    | 4640/7580 [1:58:22<1:05:32,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4640/7580 samples. Avg. Levenshtein: 192.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  62%|██████▏   | 4680/7580 [1:59:05<50:31,  1.05s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4680/7580 samples. Avg. Levenshtein: 192.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  62%|██████▏   | 4720/7580 [2:00:04<57:47,  1.21s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4720/7580 samples. Avg. Levenshtein: 191.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  63%|██████▎   | 4760/7580 [2:00:53<54:57,  1.17s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4760/7580 samples. Avg. Levenshtein: 191.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  63%|██████▎   | 4800/7580 [2:01:43<1:02:08,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4800/7580 samples. Avg. Levenshtein: 191.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  64%|██████▍   | 4840/7580 [2:03:03<1:28:32,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4840/7580 samples. Avg. Levenshtein: 190.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  64%|██████▍   | 4880/7580 [2:03:48<54:16,  1.21s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4880/7580 samples. Avg. Levenshtein: 190.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  65%|██████▍   | 4920/7580 [2:04:41<57:16,  1.29s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4920/7580 samples. Avg. Levenshtein: 190.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  65%|██████▌   | 4960/7580 [2:07:42<2:05:46,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4960/7580 samples. Avg. Levenshtein: 190.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  66%|██████▌   | 5000/7580 [2:08:23<51:39,  1.20s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5000/7580 samples. Avg. Levenshtein: 189.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  66%|██████▋   | 5040/7580 [2:09:21<57:41,  1.36s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5040/7580 samples. Avg. Levenshtein: 189.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  67%|██████▋   | 5080/7580 [2:10:16<55:43,  1.34s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5080/7580 samples. Avg. Levenshtein: 188.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  68%|██████▊   | 5120/7580 [2:11:32<1:12:51,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5120/7580 samples. Avg. Levenshtein: 188.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  68%|██████▊   | 5160/7580 [2:12:54<1:46:45,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5160/7580 samples. Avg. Levenshtein: 188.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  69%|██████▊   | 5200/7580 [2:14:03<1:25:46,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5200/7580 samples. Avg. Levenshtein: 189.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  69%|██████▉   | 5240/7580 [2:14:55<55:46,  1.43s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5240/7580 samples. Avg. Levenshtein: 189.19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  70%|██████▉   | 5280/7580 [2:15:56<54:36,  1.42s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5280/7580 samples. Avg. Levenshtein: 188.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  70%|███████   | 5320/7580 [2:16:43<43:16,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5320/7580 samples. Avg. Levenshtein: 188.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  71%|███████   | 5360/7580 [2:18:00<1:13:00,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5360/7580 samples. Avg. Levenshtein: 188.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  71%|███████   | 5400/7580 [2:19:22<1:26:09,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5400/7580 samples. Avg. Levenshtein: 189.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  72%|███████▏  | 5440/7580 [2:20:20<1:05:03,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5440/7580 samples. Avg. Levenshtein: 189.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  72%|███████▏  | 5480/7580 [2:21:09<43:09,  1.23s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5480/7580 samples. Avg. Levenshtein: 189.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  73%|███████▎  | 5520/7580 [2:22:03<44:52,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5520/7580 samples. Avg. Levenshtein: 189.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  73%|███████▎  | 5560/7580 [2:23:06<52:58,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5560/7580 samples. Avg. Levenshtein: 189.08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  74%|███████▍  | 5600/7580 [2:23:49<41:26,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5600/7580 samples. Avg. Levenshtein: 188.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  74%|███████▍  | 5640/7580 [2:24:54<41:54,  1.30s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5640/7580 samples. Avg. Levenshtein: 188.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  75%|███████▍  | 5680/7580 [2:25:58<51:57,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5680/7580 samples. Avg. Levenshtein: 188.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  75%|███████▌  | 5720/7580 [2:28:31<1:11:44,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5720/7580 samples. Avg. Levenshtein: 188.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  76%|███████▌  | 5760/7580 [2:29:31<54:00,  1.78s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5760/7580 samples. Avg. Levenshtein: 188.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  77%|███████▋  | 5800/7580 [2:31:02<1:01:48,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5800/7580 samples. Avg. Levenshtein: 189.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  77%|███████▋  | 5840/7580 [2:32:01<42:29,  1.47s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5840/7580 samples. Avg. Levenshtein: 189.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  78%|███████▊  | 5880/7580 [2:32:40<32:32,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5880/7580 samples. Avg. Levenshtein: 189.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  78%|███████▊  | 5920/7580 [2:33:22<24:59,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5920/7580 samples. Avg. Levenshtein: 188.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  79%|███████▊  | 5960/7580 [2:34:13<29:32,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5960/7580 samples. Avg. Levenshtein: 188.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  79%|███████▉  | 6000/7580 [2:35:19<41:52,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 6000/7580 samples. Avg. Levenshtein: 188.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  80%|███████▉  | 6040/7580 [2:36:11<33:55,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 6040/7580 samples. Avg. Levenshtein: 187.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  80%|████████  | 6080/7580 [2:38:41<1:24:20,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 6080/7580 samples. Avg. Levenshtein: 188.19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  81%|████████  | 6120/7580 [2:40:28<1:11:51,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 6120/7580 samples. Avg. Levenshtein: 188.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  81%|████████▏ | 6160/7580 [2:41:37<46:40,  1.97s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 6160/7580 samples. Avg. Levenshtein: 188.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  82%|████████▏ | 6200/7580 [2:42:44<35:36,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 6200/7580 samples. Avg. Levenshtein: 188.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  82%|████████▏ | 6240/7580 [2:44:33<40:59,  1.84s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 6240/7580 samples. Avg. Levenshtein: 190.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  83%|████████▎ | 6280/7580 [2:45:58<43:53,  2.03s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 6280/7580 samples. Avg. Levenshtein: 190.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  83%|████████▎ | 6320/7580 [2:46:50<28:56,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 6320/7580 samples. Avg. Levenshtein: 190.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  84%|████████▍ | 6360/7580 [2:48:02<34:00,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 6360/7580 samples. Avg. Levenshtein: 190.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  84%|████████▍ | 6400/7580 [2:48:57<29:11,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 6400/7580 samples. Avg. Levenshtein: 190.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  85%|████████▍ | 6440/7580 [2:49:58<30:18,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 6440/7580 samples. Avg. Levenshtein: 190.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  85%|████████▌ | 6480/7580 [2:50:58<31:28,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 6480/7580 samples. Avg. Levenshtein: 190.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  86%|████████▌ | 6520/7580 [2:52:10<36:22,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 6520/7580 samples. Avg. Levenshtein: 190.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  87%|████████▋ | 6560/7580 [2:53:03<24:47,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 6560/7580 samples. Avg. Levenshtein: 190.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  87%|████████▋ | 6600/7580 [2:56:06<1:41:02,  6.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 6600/7580 samples. Avg. Levenshtein: 191.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  88%|████████▊ | 6640/7580 [2:58:20<47:38,  3.04s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 6640/7580 samples. Avg. Levenshtein: 193.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  88%|████████▊ | 6680/7580 [2:59:29<30:00,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 6680/7580 samples. Avg. Levenshtein: 193.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  89%|████████▊ | 6720/7580 [3:00:33<23:45,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 6720/7580 samples. Avg. Levenshtein: 193.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  89%|████████▉ | 6760/7580 [3:01:28<19:50,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 6760/7580 samples. Avg. Levenshtein: 193.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  90%|████████▉ | 6800/7580 [3:02:33<21:46,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 6800/7580 samples. Avg. Levenshtein: 193.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  90%|█████████ | 6840/7580 [3:03:20<16:16,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 6840/7580 samples. Avg. Levenshtein: 193.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  91%|█████████ | 6880/7580 [3:03:58<11:48,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 6880/7580 samples. Avg. Levenshtein: 193.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  91%|█████████▏| 6920/7580 [3:05:05<16:41,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 6920/7580 samples. Avg. Levenshtein: 193.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  92%|█████████▏| 6960/7580 [3:05:54<12:14,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 6960/7580 samples. Avg. Levenshtein: 193.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  92%|█████████▏| 7000/7580 [3:06:49<13:44,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 7000/7580 samples. Avg. Levenshtein: 193.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  93%|█████████▎| 7040/7580 [3:07:51<13:57,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 7040/7580 samples. Avg. Levenshtein: 193.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  93%|█████████▎| 7080/7580 [3:09:25<16:15,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 7080/7580 samples. Avg. Levenshtein: 193.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  94%|█████████▍| 7120/7580 [3:10:30<12:24,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 7120/7580 samples. Avg. Levenshtein: 192.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  94%|█████████▍| 7160/7580 [3:11:50<16:04,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 7160/7580 samples. Avg. Levenshtein: 193.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  95%|█████████▍| 7200/7580 [3:12:40<09:11,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 7200/7580 samples. Avg. Levenshtein: 192.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  96%|█████████▌| 7240/7580 [3:13:17<04:58,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 7240/7580 samples. Avg. Levenshtein: 192.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  96%|█████████▌| 7280/7580 [3:14:06<05:57,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 7280/7580 samples. Avg. Levenshtein: 192.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  97%|█████████▋| 7320/7580 [3:15:02<05:32,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 7320/7580 samples. Avg. Levenshtein: 192.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  97%|█████████▋| 7360/7580 [3:15:50<04:21,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 7360/7580 samples. Avg. Levenshtein: 192.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  98%|█████████▊| 7400/7580 [3:17:11<05:17,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 7400/7580 samples. Avg. Levenshtein: 192.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  98%|█████████▊| 7440/7580 [3:18:10<03:26,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 7440/7580 samples. Avg. Levenshtein: 192.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  99%|█████████▊| 7480/7580 [3:18:57<02:06,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 7480/7580 samples. Avg. Levenshtein: 191.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches:  99%|█████████▉| 7520/7580 [3:19:42<01:05,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 7520/7580 samples. Avg. Levenshtein: 191.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches: 100%|█████████▉| 7560/7580 [3:20:53<00:29,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 7560/7580 samples. Avg. Levenshtein: 191.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating in batches: 100%|██████████| 7580/7580 [3:21:14<00:00,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 7580/7580 samples. Avg. Levenshtein: 191.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "modified_samples = {}\n",
    "levenshtein_distances = []\n",
    "\n",
    "print(\"Processing entire dataset...\")\n",
    "\n",
    "prompt = '''<start_of_turn>user\n",
    "Refine this Python code to be more concise and readable while preserving its exact functionality. The code’s input and output behavior must remain unchanged, regardless of input types or structure. Ensure the refined code:\n",
    "- Is valid Python with no syntax errors.\n",
    "- Maintains the same logic and results as the original.\n",
    "- Avoids introducing new dependencies or invalid assumptions (e.g., treating scalars as iterables).\n",
    "Output ONLY the refined Python code. Do NOT include explanations, comments, or any text other than the code itself:\n",
    "{code}\n",
    "<end_of_turn>\n",
    "<start_of_turn>model'''\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def left_pad(sequences, pad_token_id):\n",
    "    max_len = max(len(seq) for seq in sequences)\n",
    "    padded = []\n",
    "    for seq in sequences:\n",
    "        pad_len = max_len - len(seq)\n",
    "        padded.append(torch.cat([torch.full((pad_len,), pad_token_id, dtype=seq.dtype), seq]))\n",
    "    return torch.stack(padded)\n",
    "\n",
    "batch_size = 8\n",
    "batch_inputs = []\n",
    "batch_sample_ids = []\n",
    "batch_codes = []\n",
    "batch_targets = []\n",
    "\n",
    "for i, sample in enumerate(tqdm(dataset, desc=\"Generating in batches\")):\n",
    "    input_text = prompt.format(code=sample['code'])\n",
    "    tokenized = tokenizer(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=8192,\n",
    "        truncation=True,\n",
    "        padding=False  # manual padding\n",
    "    )\n",
    "    input_ids = tokenized['input_ids'][0]\n",
    "    attention_mask = tokenized['attention_mask'][0]\n",
    "\n",
    "    batch_inputs.append((input_ids, attention_mask))\n",
    "    batch_sample_ids.append(i)\n",
    "    batch_codes.append(sample['code'])\n",
    "    batch_targets.append(sample['target'])\n",
    "\n",
    "    if len(batch_inputs) == batch_size or i == len(dataset) - 1:\n",
    "        input_ids_padded = left_pad([x[0] for x in batch_inputs], tokenizer.pad_token_id)\n",
    "        attention_mask_padded = left_pad([x[1] for x in batch_inputs], 0)\n",
    "\n",
    "        inputs = {\n",
    "            \"input_ids\": input_ids_padded.to(device),\n",
    "            \"attention_mask\": attention_mask_padded.to(device),\n",
    "        }\n",
    "\n",
    "        input_lengths = [len(x[0]) for x in batch_inputs]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max(8192 + 50 - l for l in input_lengths),\n",
    "                min_new_tokens=20,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        for j, (sample_idx, orig_code, target, input_len) in enumerate(zip(batch_sample_ids, batch_codes, batch_targets, input_lengths)):\n",
    "            gen_tokens = outputs[j][input_len:]\n",
    "            gen_text = tokenizer.decode(gen_tokens, skip_special_tokens=True)\n",
    "            response = clean_response(gen_text)\n",
    "            distance = Levenshtein_distance(response, orig_code)\n",
    "\n",
    "            modified_samples[sample_idx] = {\n",
    "                'original_code': orig_code,\n",
    "                'modified_code': response,\n",
    "                'target': target,\n",
    "                'levenshtein_distance': distance,\n",
    "                'original_length': len(orig_code),\n",
    "                'modified_length': len(response),\n",
    "                'dataset_index': sample_idx\n",
    "            }\n",
    "\n",
    "            levenshtein_distances.append(distance)\n",
    "\n",
    "        # Clear batch\n",
    "        batch_inputs = []\n",
    "        batch_sample_ids = []\n",
    "        batch_codes = []\n",
    "        batch_targets = []\n",
    "\n",
    "        if (i + 1) % 10 == 0:\n",
    "            avg_distance = sum(levenshtein_distances) / len(levenshtein_distances)\n",
    "            print(f\"Processed {i+1}/{len(dataset)} samples. Avg. Levenshtein: {avg_distance:.2f}\")\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            try:\n",
    "                save_results(\"modified_samples_partial.json\", modified_samples)\n",
    "            except Exception as e:\n",
    "                print(f\"Error during partial save: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9990a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(\"modified_samples_partial.json\", modified_samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

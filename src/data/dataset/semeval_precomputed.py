"""
Dataset Loader for Precomputed Embeddings
==========================================

This module provides a dataset loader that works with precomputed embeddings
generated by the generate_embeddings.py script. It maintains the same interface
as the original SemEval2026Task13 class but loads precomputed embeddings instead
of raw text that needs to be tokenized.

The dataset includes all original columns (code, generator, label, language, target_binary)
plus an 'embedding' column containing the precomputed CLS token embeddings.
"""

import logging
import os
from typing import Tuple, Union, List, Optional
from datasets import Dataset, load_from_disk

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class SemEvalPrecomputed:
    """Dataset class for loading precomputed embeddings with all original columns intact."""
    
    def __init__(self, embeddings_path: str):
        """
        Initialize the precomputed embeddings dataset loader.
        
        Args:
            embeddings_path: Path to the saved embeddings dataset directory
                           (created by generate_embeddings.py)
        
        Raises:
            FileNotFoundError: If embeddings path doesn't exist
        """
        if not os.path.exists(embeddings_path):
            raise FileNotFoundError(
                f"Embeddings path not found: {embeddings_path}\n"
                f"Please generate embeddings first using generate_embeddings.py"
            )
        
        self.embeddings_path = embeddings_path
        
        # Cache for loaded datasets
        self._train = None
        self._val = None
        self._test = None
        
        logger.info(f"Initialized SemEvalPrecomputed from {embeddings_path}")
    
    def _load_full_dataset(self):
        """Load the full dataset from disk."""
        logger.info(f"Loading precomputed embeddings from {self.embeddings_path}...")
        dataset_dict = load_from_disk(self.embeddings_path)
        return dataset_dict
    
    def _load_split(self, split_name: str) -> Dataset:
        """
        Load a specific split.
        
        Args:
            split_name: 'train', 'val', or 'test'
            
        Returns:
            Dataset with embeddings
        """
        # Map common split names
        split_mapping = {
            'val': 'validation',
            'dev': 'validation'
        }
        hf_split_name = split_mapping.get(split_name, split_name)
        
        dataset_dict = self._load_full_dataset()
        
        if hf_split_name not in dataset_dict:
            available_splits = list(dataset_dict.keys())
            raise ValueError(
                f"Split '{hf_split_name}' not found. Available: {available_splits}"
            )
        
        dataset = dataset_dict[hf_split_name]
        logger.info(f"Loaded {hf_split_name} split: {len(dataset)} samples")
        
        return dataset
    
    def _get_train_subset(self, dataset: Dataset, subset_fraction: float) -> Dataset:
        """
        Get a stratified subset of the training dataset.
        
        Args:
            dataset: The full training dataset
            subset_fraction: Fraction to keep (0.0 to 1.0)
            
        Returns:
            Stratified subset
        """
        if not 0 < subset_fraction <= 1.0:
            raise ValueError("train_subset must be between 0 and 1.0")
        
        if subset_fraction == 1.0:
            return dataset
        
        try:
            subset_data = dataset.train_test_split(
                train_size=subset_fraction,
                seed=42,
                stratify_by_column='target_binary'
            )
            return subset_data['train']
        except Exception as e:
            logger.warning(f"Stratified sampling failed: {e}. Using regular sampling.")
            total_samples = len(dataset)
            subset_size = int(total_samples * subset_fraction)
            return dataset.select(range(subset_size))
    
    def _limit_split_size(self, dataset: Dataset, max_size: int) -> Dataset:
        """Limit the size of a dataset split using stratified sampling."""
        if len(dataset) <= max_size:
            return dataset
        
        fraction = max_size / len(dataset)
        
        try:
            if 'target_binary' in dataset.column_names:
                limited_data = dataset.train_test_split(
                    train_size=fraction,
                    seed=42,
                    stratify_by_column='target_binary'
                )
                return limited_data['train']
            else:
                return dataset.select(range(max_size))
        except Exception as e:
            logger.warning(f"Stratified sampling failed: {e}. Using regular sampling.")
            return dataset.select(range(max_size))
    
    def get_dataset(
        self,
        split: Union[str, List[str]] = 'all',
        train_subset: float = 1.0,
        dynamic_split_sizing: bool = False,
        max_split_ratio: float = 0.2,
        val_ratio: Optional[float] = None,
        test_ratio: Optional[float] = None
    ) -> Union[Dataset, Tuple[Dataset, ...]]:
        """
        Load precomputed embeddings dataset.
        
        Args:
            split: Split(s) to load ('train', 'val', 'test', 'all', or list)
            train_subset: Fraction of training data to use
            dynamic_split_sizing: Whether to dynamically limit val/test sizes
            max_split_ratio: Maximum ratio of val/test size to train size
            val_ratio: Specific ratio for validation set size
            test_ratio: Specific ratio for test set size
            
        Returns:
            Single dataset or tuple of datasets
        """
        valid_splits = ['train', 'val', 'test', 'all']
        
        # Handle split parameter
        return_tuple = False
        if isinstance(split, str):
            if split not in valid_splits:
                raise ValueError(f"Invalid split '{split}'. Must be one of {valid_splits}")
            splits_to_load = [split] if split != 'all' else ['train', 'val', 'test']
        else:
            return_tuple = True
            invalid_splits = [s for s in split if s not in ['train', 'val', 'test']]
            if invalid_splits:
                raise ValueError(f"Invalid splits {invalid_splits}")
            splits_to_load = split
        
        # Load datasets
        datasets = {}
        
        for split_name in splits_to_load:
            # Use cached dataset if available
            if split_name == 'train' and self._train is not None:
                datasets[split_name] = self._train
            elif split_name == 'val' and self._val is not None:
                datasets[split_name] = self._val
            elif split_name == 'test' and self._test is not None:
                datasets[split_name] = self._test
            else:
                # Load and cache
                dataset = self._load_split(split_name)
                datasets[split_name] = dataset
                
                if split_name == 'train':
                    self._train = dataset
                elif split_name == 'val':
                    self._val = dataset
                elif split_name == 'test':
                    self._test = dataset
        
        # Apply train subset if needed
        if 'train' in datasets and train_subset < 1.0:
            datasets['train'] = self._get_train_subset(datasets['train'], train_subset)
            self._train = datasets['train']
        
        # Apply dynamic sizing if requested
        if dynamic_split_sizing and 'train' in datasets:
            train_size = len(datasets['train'])
            
            for split_name in ['val', 'test']:
                if split_name in datasets:
                    original_size = len(datasets[split_name])
                    
                    if split_name == 'val' and val_ratio is not None:
                        target_size = int(original_size * val_ratio)
                    elif split_name == 'test' and test_ratio is not None:
                        target_size = int(original_size * test_ratio)
                    else:
                        target_size = int(train_size * max_split_ratio)
                    
                    if target_size < original_size:
                        datasets[split_name] = self._limit_split_size(
                            datasets[split_name], target_size
                        )
                        logger.info(
                            f"Limited {split_name} from {original_size} to "
                            f"{len(datasets[split_name])} samples"
                        )
        
        # Return results
        if return_tuple:
            return tuple(datasets[split_name] for split_name in splits_to_load)
        elif len(splits_to_load) == 1:
            return datasets[splits_to_load[0]]
        else:
            from datasets import concatenate_datasets
            return concatenate_datasets(
                [datasets[split_name] for split_name in splits_to_load]
            )
    
    def get_info(self) -> dict:
        """Get information about the precomputed embeddings dataset."""
        info = {
            'embeddings_path': self.embeddings_path,
            'type': 'precomputed_embeddings'
        }
        
        # Add size information if cached
        if self._train is not None:
            info['train_size'] = len(self._train)
        if self._val is not None:
            info['val_size'] = len(self._val)
        if self._test is not None:
            info['test_size'] = len(self._test)
        
        # Try to load to get more info
        try:
            dataset_dict = self._load_full_dataset()
            info['available_splits'] = list(dataset_dict.keys())
            
            for split_name in dataset_dict.keys():
                if f'{split_name}_size' not in info:
                    info[f'{split_name}_size'] = len(dataset_dict[split_name])
                info[f'{split_name}_columns'] = dataset_dict[split_name].column_names
                
                # Get embedding dimension from first example
                if 'embedding' in dataset_dict[split_name].column_names:
                    first_embedding = dataset_dict[split_name][0]['embedding']
                    info['embedding_dim'] = len(first_embedding)
        except Exception as e:
            logger.warning(f"Could not load dataset for info: {e}")
        
        return info


if __name__ == "__main__":
    # Example usage
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: python semeval_precomputed.py <embeddings_path>")
        print("Example: python semeval_precomputed.py data/embeddings/semeval_codebert_embeddings")
        sys.exit(1)
    
    embeddings_path = sys.argv[1]
    
    # Initialize loader
    loader = SemEvalPrecomputed(embeddings_path)
    
    # Get info
    info = loader.get_info()
    logger.info(f"Dataset info: {info}")
    
    # Load splits
    train, val, test = loader.get_dataset(split=['train', 'val', 'test'])
    
    logger.info(f"Train size: {len(train)}")
    logger.info(f"Val size: {len(val)}")
    logger.info(f"Test size: {len(test)}")
    
    # Show sample
    sample = train[0]
    logger.info(f"Sample keys: {list(sample.keys())}")
    logger.info(f"Embedding shape: {len(sample['embedding'])}")
    logger.info(f"Code snippet: {sample['code'][:100]}...")
    logger.info(f"Label: {sample['target_binary']}")

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66a2740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c16d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataset.codet_m4_cleaned import CoDeTM4Cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bb9f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test  = CoDeTM4Cleaned('../../data/codet_cleaned_20250812_201438/').get_dataset(['train','val','test'], columns='all', dynamic_split_sizing=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5d9983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e84a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "codet = concatenate_datasets([train, val, test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455df506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tree_sitter_python as tspython\n",
    "import tree_sitter_cpp as tscpp\n",
    "import tree_sitter_java as tsjava\n",
    "from tree_sitter import Parser, Language\n",
    "\n",
    "TS_PYTHON = Language(tspython.language())\n",
    "TS_JAVA = Language(tsjava.language())\n",
    "TS_CPP = Language(tscpp.language())\n",
    "\n",
    "PYTHON_PARSER, JAVA_PARSER, CPP_PARSER = Parser(language=TS_PYTHON), Parser(language=TS_JAVA), Parser(language=TS_CPP)\n",
    "\n",
    "# Enable comment parsing by setting `parser.set_included_ranges` with full range of the source code\n",
    "# This is a workaround: tree-sitter parsers by default include comments as nodes, \n",
    "# so no extra flag is needed, but if previously you filtered comments, do not filter now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ad5b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c287834",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_counts = codet['language']\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.hist(language_counts, bins=len(set(language_counts)), edgecolor='black')\n",
    "plt.xlabel('Language')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Programming Languages in CoDeTM4 Cleaned Dataset')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "python_count = language_counts.count(\"python\")\n",
    "java_count = language_counts.count(\"java\")\n",
    "cpp_count = language_counts.count(\"cpp\")\n",
    "\n",
    "print(f'Python count: {python_count}')\n",
    "print(f'Java count: {java_count}')\n",
    "print(f'C++ count: {cpp_count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2617ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parser(language):\n",
    "    match language:\n",
    "        case 'python':\n",
    "            return PYTHON_PARSER\n",
    "        case 'java':\n",
    "            return JAVA_PARSER\n",
    "        case 'cpp':\n",
    "            return CPP_PARSER\n",
    "    raise ValueError(f\"Unsupported language: {language}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b74c5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tree(sample, code_key='cleaned_code'):\n",
    "    language = sample['language']\n",
    "    parser = get_parser(language)\n",
    "    # Include full range to keep comments included\n",
    "    code_bytes = bytes(sample[code_key], 'utf-8')\n",
    "    # Use full range\n",
    "    #parser.set_included_ranges([ (0, len(code_bytes)) ])\n",
    "    tree = parser.parse(code_bytes)\n",
    "    # Reset included ranges to None after parse to avoid side effects\n",
    "    #parser.set_included_ranges(None)\n",
    "    return tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8794468c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tree_sitter import TreeCursor\n",
    "\n",
    "def walk_tree(cursor: TreeCursor, depth=0):\n",
    "    indent = '  ' * depth\n",
    "    print(f\"{indent}{cursor.node.type}\")\n",
    "\n",
    "    if cursor.goto_first_child():\n",
    "        walk_tree(cursor, depth+1)\n",
    "    \n",
    "        while cursor.goto_next_sibling():\n",
    "            walk_tree(cursor, depth+1)\n",
    "\n",
    "        cursor.goto_parent()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d5c69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Set\n",
    "\n",
    "def get_node_types_from_tree(cursor: TreeCursor, types: Set[str]=None) -> Set[str]:\n",
    "    if types is None:\n",
    "        types = set()\n",
    "    \n",
    "    types.add(cursor.node.type)\n",
    "\n",
    "    if cursor.goto_first_child():\n",
    "        get_node_types_from_tree(cursor, types)\n",
    "    \n",
    "        while cursor.goto_next_sibling():\n",
    "            get_node_types_from_tree(cursor, types)\n",
    "\n",
    "        cursor.goto_parent()\n",
    "\n",
    "    return types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100d18a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_types(sample):\n",
    "    # Import everything needed inside the function\n",
    "    import tree_sitter_python as tspython\n",
    "    import tree_sitter_cpp as tscpp\n",
    "    import tree_sitter_java as tsjava\n",
    "    from tree_sitter import Parser, Language, TreeCursor\n",
    "    from typing import Set\n",
    "    \n",
    "    # Create parsers locally\n",
    "    TS_PYTHON = Language(tspython.language())\n",
    "    TS_JAVA = Language(tsjava.language())\n",
    "    TS_CPP = Language(tscpp.language())\n",
    "    \n",
    "    PYTHON_PARSER = Parser(language=TS_PYTHON)\n",
    "    JAVA_PARSER = Parser(language=TS_JAVA)\n",
    "    CPP_PARSER = Parser(language=TS_CPP)\n",
    "    \n",
    "    def get_parser(language):\n",
    "        match language:\n",
    "            case 'python':\n",
    "                return PYTHON_PARSER\n",
    "            case 'java':\n",
    "                return JAVA_PARSER\n",
    "            case 'cpp':\n",
    "                return CPP_PARSER\n",
    "        raise ValueError(f\"Unsupported language: {language}\")\n",
    "    \n",
    "    def get_node_types_from_tree(cursor: TreeCursor, types: Set[str]=None) -> Set[str]:\n",
    "        if types is None:\n",
    "            types = set()\n",
    "        \n",
    "        types.add(cursor.node.type)\n",
    "\n",
    "        if cursor.goto_first_child():\n",
    "            get_node_types_from_tree(cursor, types)\n",
    "        \n",
    "            while cursor.goto_next_sibling():\n",
    "                get_node_types_from_tree(cursor, types)\n",
    "\n",
    "            cursor.goto_parent()\n",
    "\n",
    "        return types\n",
    "    \n",
    "    parser = get_parser(sample['language'])\n",
    "    code_bytes = sample['code'].encode('utf-8')\n",
    "    # parser.set_included_ranges([ (0, len(code_bytes)) ])\n",
    "    tree = parser.parse(code_bytes)\n",
    "    # parser.set_included_ranges(None)\n",
    "    cursor = tree.walk()\n",
    "    types = get_node_types_from_tree(cursor)\n",
    "    return {\"types\": list(types)}\n",
    "\n",
    "result = codet.map(extract_types, batched=False, num_proc=8)\n",
    "\n",
    "all_types = set()\n",
    "for tlist in result['types']:\n",
    "    all_types.update(tlist)\n",
    "\n",
    "print(f\"Collected {len(all_types)} unique node types\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76565561",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_types = sorted(list(all_types))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c358a009",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_to_ind = {t: i for i, t in enumerate(all_types)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111c27bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8036482f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in all_types:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34be9768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from typing import List, Tuple, Dict\n",
    "from tree_sitter import Node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61db55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from typing import List, Tuple, Dict\n",
    "from torch import tensor, long as tlong\n",
    "from tree_sitter import TreeCursor\n",
    "\n",
    "def tree_to_graph(cursor: TreeCursor, id_map: Dict = None, next_id: int = 0, edges: List[Tuple[int, int]] = None) -> Tuple[List[Tuple[int, int]], Dict, int]:\n",
    "    if edges is None:\n",
    "        edges = []\n",
    "    if id_map is None:\n",
    "        id_map = {}\n",
    "\n",
    "    # Assign ID to current node\n",
    "    if cursor.node not in id_map:\n",
    "        id_map[cursor.node] = next_id\n",
    "        next_id += 1\n",
    "    current_id = id_map[cursor.node]\n",
    "\n",
    "    if cursor.goto_first_child():\n",
    "        # Process first child\n",
    "        if cursor.node not in id_map:\n",
    "            id_map[cursor.node] = next_id\n",
    "            next_id += 1\n",
    "        child_id = id_map[cursor.node]\n",
    "        edges.append((current_id, child_id))\n",
    "        edges, id_map, next_id = tree_to_graph(cursor, id_map, next_id, edges)\n",
    "        \n",
    "        # Process siblings\n",
    "        while cursor.goto_next_sibling():\n",
    "            if cursor.node not in id_map:\n",
    "                id_map[cursor.node] = next_id\n",
    "                next_id += 1\n",
    "            child_id = id_map[cursor.node]\n",
    "            edges.append((current_id, child_id))\n",
    "            edges, id_map, next_id = tree_to_graph(cursor, id_map, next_id, edges)\n",
    "        \n",
    "        cursor.goto_parent()\n",
    "\n",
    "    return edges, id_map, next_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c404d5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765930d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(sample, code_key='cleaned_code'):\n",
    "    tree = create_tree(sample, code_key=code_key)\n",
    "    edges, id_map, _ = tree_to_graph(tree.walk())\n",
    "    edge_index = tensor(edges, dtype=tlong).t().contiguous()\n",
    "    x = [type_to_ind[node.type] for node, _ in sorted(id_map.items(), key=lambda kv: kv[1])]\n",
    "    x = tensor(x, dtype=tlong)\n",
    "    y = tensor([sample['target_binary']], dtype=tlong)\n",
    "\n",
    "    graph_features = tensor(list(sample['features'].values()))\n",
    "    \n",
    "    metadata = {\n",
    "        'language': sample['language'],\n",
    "        'target': sample['target'],\n",
    "        'target_binary': sample['target_binary'],\n",
    "        'code': sample['code'],\n",
    "        'cleaned_code': sample['cleaned_code']\n",
    "    }\n",
    "    \n",
    "    data = Data(\n",
    "        x=x, \n",
    "        y=y, \n",
    "        edge_index=edge_index, \n",
    "        graph_features=graph_features,\n",
    "        metadata=metadata\n",
    "    )    \n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6641b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "codet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d201b1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch import tensor, long as tlong\n",
    "from typing import List, Dict, Tuple\n",
    "from collections import defaultdict, deque\n",
    "import torch\n",
    "def compute_depths(num_nodes: int, edges: List[Tuple[int, int]]) -> tlong:\n",
    "    \"\"\"Compute depth (distance from root) for each node.\"\"\"\n",
    "    depths = torch.zeros(num_nodes, dtype=torch.long)\n",
    "    tree = defaultdict(list)\n",
    "    for parent, child in edges:\n",
    "        tree[parent].append(child)\n",
    "    visited = [False] * num_nodes\n",
    "    queue = deque([0])  # assume root node has ID 0\n",
    "    visited[0] = True\n",
    "    while queue:\n",
    "        node = queue.popleft()\n",
    "        for child in tree[node]:\n",
    "            if not visited[child]:\n",
    "                depths[child] = depths[node] + 1\n",
    "                visited[child] = True\n",
    "                queue.append(child)\n",
    "    return depths\n",
    "\n",
    "def compute_child_indices(num_nodes: int, edges: List[Tuple[int, int]]) -> tlong:\n",
    "    \"\"\"Compute sibling index for each node (order among its siblings).\"\"\"\n",
    "    child_idx = torch.zeros(num_nodes, dtype=torch.long)\n",
    "    tree = defaultdict(list)\n",
    "    for parent, child in edges:\n",
    "        tree[parent].append(child)\n",
    "    for parent, children in tree.items():\n",
    "        for i, child in enumerate(children):\n",
    "            child_idx[child] = i\n",
    "    return child_idx\n",
    "\n",
    "def create_graph(sample, code_key='cleaned_code'):\n",
    "    \"\"\"Create a PyG Data object with node type, depth, and child index embeddings.\"\"\"\n",
    "    tree = create_tree(sample, code_key=code_key)\n",
    "    edges, id_map, _ = tree_to_graph(tree.walk())\n",
    "    \n",
    "    edge_index = tensor(edges, dtype=tlong).t().contiguous()\n",
    "    \n",
    "    # Node type IDs\n",
    "    x = [type_to_ind[node.type] for node, _ in sorted(id_map.items(), key=lambda kv: kv[1])]\n",
    "    x = tensor(x, dtype=tlong)\n",
    "    num_nodes = x.size(0)\n",
    "\n",
    "    # Compute depth and child index\n",
    "    node_depth = compute_depths(num_nodes, edges)\n",
    "    child_index = compute_child_indices(num_nodes, edges)\n",
    "    \n",
    "    # Target\n",
    "    y = tensor([sample['target_binary']], dtype=tlong)\n",
    "    \n",
    "    # Graph features\n",
    "    graph_features = tensor(list(sample['features'].values()))\n",
    "    \n",
    "    # Metadata\n",
    "    metadata = {\n",
    "        'language': sample['language'],\n",
    "        'target': sample['target'],\n",
    "        'target_binary': sample['target_binary'],\n",
    "        'code': sample['code'],\n",
    "        'cleaned_code': sample['cleaned_code']\n",
    "    }\n",
    "\n",
    "    data = Data(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        edge_index=edge_index,\n",
    "        node_depth=node_depth,\n",
    "        child_index=child_index,\n",
    "        graph_features=graph_features,\n",
    "        metadata=metadata\n",
    "    )\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def create_graphs(dataset, desc_keyword, code_key='cleaned_code'):\n",
    "    graphs = []\n",
    "\n",
    "    for i, sample in enumerate(tqdm(dataset, desc=f'Creating {desc_keyword} graphs')):\n",
    "        data = create_graph(sample, code_key)\n",
    "        graphs.append(data)\n",
    "\n",
    "    return graphs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd57b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics tracking for depth and child indices\n",
    "max_depth_global = 0\n",
    "max_child_index_global = 0\n",
    "depth_stats = []\n",
    "child_index_stats = []\n",
    "\n",
    "def compute_depths(num_nodes: int, edges: List[Tuple[int, int]]) -> tlong:\n",
    "    \"\"\"Compute depth (distance from root) for each node.\"\"\"\n",
    "    global max_depth_global\n",
    "    depths = torch.zeros(num_nodes, dtype=torch.long)\n",
    "    tree = defaultdict(list)\n",
    "    for parent, child in edges:\n",
    "        tree[parent].append(child)\n",
    "    visited = [False] * num_nodes\n",
    "    queue = deque([0])  # assume root node has ID 0\n",
    "    visited[0] = True\n",
    "    while queue:\n",
    "        node = queue.popleft()\n",
    "        for child in tree[node]:\n",
    "            if not visited[child]:\n",
    "                depths[child] = depths[node] + 1\n",
    "                visited[child] = True\n",
    "                queue.append(child)\n",
    "    \n",
    "    # Update global max depth\n",
    "    current_max_depth = depths.max().item()\n",
    "    max_depth_global = max(max_depth_global, current_max_depth)\n",
    "    \n",
    "    return depths\n",
    "\n",
    "def compute_child_indices(num_nodes: int, edges: List[Tuple[int, int]]) -> tlong:\n",
    "    \"\"\"Compute sibling index for each node (order among its siblings).\"\"\"\n",
    "    global max_child_index_global\n",
    "    child_idx = torch.zeros(num_nodes, dtype=torch.long)\n",
    "    tree = defaultdict(list)\n",
    "    for parent, child in edges:\n",
    "        tree[parent].append(child)\n",
    "    for parent, children in tree.items():\n",
    "        for i, child in enumerate(children):\n",
    "            child_idx[child] = i\n",
    "    \n",
    "    # Update global max child index\n",
    "    if len(child_idx) > 0:\n",
    "        current_max_child_index = child_idx.max().item()\n",
    "        max_child_index_global = max(max_child_index_global, current_max_child_index)\n",
    "    \n",
    "    return child_idx\n",
    "\n",
    "def create_graph(sample, code_key='cleaned_code'):\n",
    "    \"\"\"Create a PyG Data object with node type, depth, and child index embeddings.\"\"\"\n",
    "    global depth_stats, child_index_stats\n",
    "    \n",
    "    tree = create_tree(sample, code_key=code_key)\n",
    "    edges, id_map, _ = tree_to_graph(tree.walk())\n",
    "    \n",
    "    edge_index = tensor(edges, dtype=tlong).t().contiguous()\n",
    "    \n",
    "    # Node type IDs\n",
    "    x = [type_to_ind[node.type] for node, _ in sorted(id_map.items(), key=lambda kv: kv[1])]\n",
    "    x = tensor(x, dtype=tlong)\n",
    "    num_nodes = x.size(0)\n",
    "\n",
    "    # Compute depth and child index\n",
    "    node_depth = compute_depths(num_nodes, edges)\n",
    "    child_index = compute_child_indices(num_nodes, edges)\n",
    "    \n",
    "    # Collect statistics for this graph\n",
    "    if len(node_depth) > 0:\n",
    "        graph_max_depth = node_depth.max().item()\n",
    "        depth_stats.append(graph_max_depth)\n",
    "    \n",
    "    if len(child_index) > 0:\n",
    "        graph_max_child_index = child_index.max().item()\n",
    "        child_index_stats.append(graph_max_child_index)\n",
    "    \n",
    "    # Target\n",
    "    y = tensor([sample['target_binary']], dtype=tlong)\n",
    "    \n",
    "    # Graph features\n",
    "    graph_features = tensor(list(sample['features'].values()))\n",
    "    \n",
    "    # Metadata\n",
    "    metadata = {\n",
    "        'language': sample['language'],\n",
    "        'target': sample['target'],\n",
    "        'target_binary': sample['target_binary'],\n",
    "        'code': sample['code'],\n",
    "        'cleaned_code': sample['cleaned_code']\n",
    "    }\n",
    "\n",
    "    data = Data(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        edge_index=edge_index,\n",
    "        node_depth=node_depth,\n",
    "        child_index=child_index,\n",
    "        graph_features=graph_features,\n",
    "        metadata=metadata\n",
    "    )\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def create_graphs(dataset, desc_keyword, code_key='cleaned_code'):\n",
    "    graphs = []\n",
    "\n",
    "    for i, sample in enumerate(tqdm(dataset, desc=f'Creating {desc_keyword} graphs')):\n",
    "        data = create_graph(sample, code_key)\n",
    "        graphs.append(data)\n",
    "\n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57279252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset statistics before creating graphs\n",
    "max_depth_global = 0\n",
    "max_child_index_global = 0\n",
    "depth_stats = []\n",
    "child_index_stats = []\n",
    "\n",
    "print(\"Starting graph creation with statistics tracking...\")\n",
    "print(\"This will track maximum depth and child index values across all graphs.\")\n",
    "print()\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch import save\n",
    "import gc\n",
    "\n",
    "train_graphs = create_graphs(train, 'train', 'code')\n",
    "save(train_graphs, '../../data/codet_graphs/train_graphs_cleaned_comments_depth.pt')\n",
    "print(f\"After train graphs: max_depth={max_depth_global}, max_child_index={max_child_index_global}\")\n",
    "del train, train_graphs\n",
    "gc.collect()\n",
    "\n",
    "val_graphs = create_graphs(val, 'val', 'code')\n",
    "save(val_graphs, '../../data/codet_graphs/val_graphs_cleaned_comments_depth.pt')\n",
    "print(f\"After val graphs: max_depth={max_depth_global}, max_child_index={max_child_index_global}\")\n",
    "del val, val_graphs\n",
    "gc.collect()\n",
    "\n",
    "test_graphs = create_graphs(test, 'test', 'code')\n",
    "save(test_graphs, '../../data/codet_graphs/test_graphs_cleaned_comments_depth.pt')\n",
    "print(f\"After test graphs: max_depth={max_depth_global}, max_child_index={max_child_index_global}\")\n",
    "del test, test_graphs\n",
    "gc.collect()\n",
    "\n",
    "save(type_to_ind, '../../data/codet_graphs/type_to_ind_cleaned_comments_depth.pt')\n",
    "\n",
    "print()\n",
    "print(\"Graph creation completed!\")\n",
    "print(f\"Final statistics: max_depth={max_depth_global}, max_child_index={max_child_index_global}\")\n",
    "print(f\"Total graphs processed: {len(depth_stats)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d3e7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save statistics about depth and child indices\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Calculate comprehensive statistics\n",
    "depth_stats_array = np.array(depth_stats)\n",
    "child_index_stats_array = np.array(child_index_stats)\n",
    "\n",
    "statistics = {\n",
    "    'max_depth_global': int(max_depth_global),\n",
    "    'max_child_index_global': int(max_child_index_global),\n",
    "    'total_graphs_processed': len(depth_stats),\n",
    "    'depth_statistics': {\n",
    "        'mean': float(np.mean(depth_stats_array)) if len(depth_stats_array) > 0 else 0.0,\n",
    "        'std': float(np.std(depth_stats_array)) if len(depth_stats_array) > 0 else 0.0,\n",
    "        'min': int(np.min(depth_stats_array)) if len(depth_stats_array) > 0 else 0,\n",
    "        'max': int(np.max(depth_stats_array)) if len(depth_stats_array) > 0 else 0,\n",
    "        'percentile_95': float(np.percentile(depth_stats_array, 95)) if len(depth_stats_array) > 0 else 0.0,\n",
    "        'percentile_99': float(np.percentile(depth_stats_array, 99)) if len(depth_stats_array) > 0 else 0.0\n",
    "    },\n",
    "    'child_index_statistics': {\n",
    "        'mean': float(np.mean(child_index_stats_array)) if len(child_index_stats_array) > 0 else 0.0,\n",
    "        'std': float(np.std(child_index_stats_array)) if len(child_index_stats_array) > 0 else 0.0,\n",
    "        'min': int(np.min(child_index_stats_array)) if len(child_index_stats_array) > 0 else 0,\n",
    "        'max': int(np.max(child_index_stats_array)) if len(child_index_stats_array) > 0 else 0,\n",
    "        'percentile_95': float(np.percentile(child_index_stats_array, 95)) if len(child_index_stats_array) > 0 else 0.0,\n",
    "        'percentile_99': float(np.percentile(child_index_stats_array, 99)) if len(child_index_stats_array) > 0 else 0.0\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save statistics to JSON file\n",
    "stats_file_path = '../../data/codet_graphs/depth_child_index_stats_cleaned_comments_depth.json'\n",
    "with open(stats_file_path, 'w') as f:\n",
    "    json.dump(statistics, f, indent=2)\n",
    "\n",
    "print(\"Graph Statistics Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total graphs processed: {statistics['total_graphs_processed']}\")\n",
    "print(f\"Global maximum depth: {statistics['max_depth_global']}\")\n",
    "print(f\"Global maximum child index: {statistics['max_child_index_global']}\")\n",
    "print()\n",
    "print(\"Depth Statistics:\")\n",
    "print(f\"  Mean: {statistics['depth_statistics']['mean']:.2f}\")\n",
    "print(f\"  Std:  {statistics['depth_statistics']['std']:.2f}\")\n",
    "print(f\"  Min:  {statistics['depth_statistics']['min']}\")\n",
    "print(f\"  Max:  {statistics['depth_statistics']['max']}\")\n",
    "print(f\"  95th percentile: {statistics['depth_statistics']['percentile_95']:.2f}\")\n",
    "print(f\"  99th percentile: {statistics['depth_statistics']['percentile_99']:.2f}\")\n",
    "print()\n",
    "print(\"Child Index Statistics:\")\n",
    "print(f\"  Mean: {statistics['child_index_statistics']['mean']:.2f}\")\n",
    "print(f\"  Std:  {statistics['child_index_statistics']['std']:.2f}\")\n",
    "print(f\"  Min:  {statistics['child_index_statistics']['min']}\")\n",
    "print(f\"  Max:  {statistics['child_index_statistics']['max']}\")\n",
    "print(f\"  95th percentile: {statistics['child_index_statistics']['percentile_95']:.2f}\")\n",
    "print(f\"  99th percentile: {statistics['child_index_statistics']['percentile_99']:.2f}\")\n",
    "print()\n",
    "print(f\"Statistics saved to: {stats_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf27384f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize depth and child index distributions\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Depth distribution\n",
    "if len(depth_stats) > 0:\n",
    "    ax1.hist(depth_stats, bins=50, alpha=0.7, edgecolor='black')\n",
    "    ax1.axvline(np.mean(depth_stats), color='red', linestyle='--', \n",
    "                label=f'Mean: {np.mean(depth_stats):.2f}')\n",
    "    ax1.axvline(np.percentile(depth_stats, 95), color='orange', linestyle='--', \n",
    "                label=f'95th percentile: {np.percentile(depth_stats, 95):.2f}')\n",
    "    ax1.set_xlabel('Maximum Depth per Graph')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.set_title('Distribution of Maximum Depths')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Child index distribution\n",
    "if len(child_index_stats) > 0:\n",
    "    ax2.hist(child_index_stats, bins=50, alpha=0.7, edgecolor='black', color='green')\n",
    "    ax2.axvline(np.mean(child_index_stats), color='red', linestyle='--', \n",
    "                label=f'Mean: {np.mean(child_index_stats):.2f}')\n",
    "    ax2.axvline(np.percentile(child_index_stats, 95), color='orange', linestyle='--', \n",
    "                label=f'95th percentile: {np.percentile(child_index_stats, 95):.2f}')\n",
    "    ax2.set_xlabel('Maximum Child Index per Graph')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.set_title('Distribution of Maximum Child Indices')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional statistics breakdown by language if possible\n",
    "print(\"\\nDetailed Analysis:\")\n",
    "print(f\"Depth range: {min(depth_stats)} to {max(depth_stats)}\")\n",
    "print(f\"Child index range: {min(child_index_stats)} to {max(child_index_stats)}\")\n",
    "print()\n",
    "print(\"These statistics help determine appropriate embedding dimensions for:\")\n",
    "print(\"- Node depth embeddings (should be >= max_depth + 1)\")\n",
    "print(\"- Child index embeddings (should be >= max_child_index + 1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6accd347",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf982ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df55eb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graphs = load('../../data/codet_graphs/train_graphs_cleaned_comments_depth.pt', weights_only=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a182de",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_to_ind = load('../../data/codet_graphs/type_to_ind_cleaned_comments_depth.pt', weights_only=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e84c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_graphs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f55559",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "def visualize_graph(data, figsize=(15, 10), show_labels=True):\n",
    "    # Convert to NetworkX graph\n",
    "    G = to_networkx(data, to_undirected=False)\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Create tree layout manually\n",
    "    pos = {}\n",
    "    \n",
    "    # Find root (node with no incoming edges)\n",
    "    root = 0\n",
    "    for node in G.nodes():\n",
    "        if G.in_degree(node) == 0:\n",
    "            root = node\n",
    "            break\n",
    "    \n",
    "    # Assign positions level by level\n",
    "    levels = {}\n",
    "    queue = [(root, 0)]\n",
    "    \n",
    "    while queue:\n",
    "        node, level = queue.pop(0)\n",
    "        levels[node] = level\n",
    "        \n",
    "        # Add children to next level\n",
    "        for child in G.successors(node):\n",
    "            queue.append((child, level + 1))\n",
    "    \n",
    "    # Group nodes by level\n",
    "    level_groups = {}\n",
    "    for node, level in levels.items():\n",
    "        if level not in level_groups:\n",
    "            level_groups[level] = []\n",
    "        level_groups[level].append(node)\n",
    "    \n",
    "    # Position nodes\n",
    "    for level, nodes in level_groups.items():\n",
    "        for i, node in enumerate(nodes):\n",
    "            x = i - len(nodes) / 2  # Center nodes horizontally\n",
    "            y = -level  # Higher levels at top\n",
    "            pos[node] = (x, y)\n",
    "    \n",
    "    # Create labels if requested\n",
    "    labels = None\n",
    "    if show_labels:\n",
    "        # Create reverse mapping from index to type\n",
    "        ind_to_type = {v: k for k, v in type_to_ind.items()}\n",
    "        labels = {}\n",
    "        for node in G.nodes():\n",
    "            node_type_idx = data.x[node].item()\n",
    "            node_type = ind_to_type.get(node_type_idx, f\"idx_{node_type_idx}\")\n",
    "            labels[node] = node_type\n",
    "    \n",
    "    nx.draw(G, pos, with_labels=show_labels, labels=labels, \n",
    "            node_color='lightblue', node_size=500, arrows=True, \n",
    "            font_size=6 if show_labels else 8)\n",
    "    \n",
    "    plt.title(\"AST Tree\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3363fb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graphs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cad8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_graph(train_graphs[1], show_labels=True)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

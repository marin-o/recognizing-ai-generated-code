{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adbbd2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from dotenv import load_dotenv\n",
    "from data.aigcodeset import AIGCodeSet\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "from typing import Tuple, Union, Dict, List\n",
    "from datasets import load_dataset, Dataset, ClassLabel\n",
    "from modelscope import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from Levenshtein import distance as Levenshtein_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "404f0427",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d072a367",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1653947a",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7b562e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "2025-06-23 17:45:40,636 - WARNING - Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "if hf_token:\n",
    "    from huggingface_hub import login\n",
    "    login(hf_token)\n",
    "else:\n",
    "    print(\"Still not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2030f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "2025-06-23 17:46:02,315 - WARNING - Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(hf_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9abb225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /home/bosa/.cache/modelscope/hub/models/AI-ModelScope/CodeLlama-7b-Instruct-hf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 17:47:35,987 - modelscope - INFO - Got 3 files, start to download ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc5fce96fce0490699b89904928cb171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing 3 items:   0%|          | 0.00/3.00 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16b50411859c448999dbc836a7e7dd19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [special_tokens_map.json]:   0%|          | 0.00/411 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01f2f2d6644f4a8680193e1c5b7fb34c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [tokenizer.json]:   0%|          | 0.00/1.76M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3639d8e9a294463996dc2a7d47d286d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [tokenizer_config.json]:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 17:47:37,861 - modelscope - INFO - Download model 'AI-ModelScope/CodeLlama-7b-Instruct-hf' successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /home/bosa/.cache/modelscope/hub/models/AI-ModelScope/CodeLlama-7b-Instruct-hf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 17:47:40,767 - modelscope - INFO - Got 14 files, start to download ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "131eadcaa55749f2ad58cdd2b4720643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing 14 items:   0%|          | 0.00/14.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d48c2eb67142421eabe97129eb671a21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [LICENSE]:   0%|          | 0.00/6.86k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fc9cba41de34a2a879eecc625bfad6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [configuration.json]:   0%|          | 0.00/134 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d690342db030441eb58e3c990e8cdad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [model-00001-of-00002.safetensors]:   0%|          | 0.00/9.29G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74703a6ab54849b2b9f39a5c2e4bee6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [config.json]:   0%|          | 0.00/646 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "076ad4f50f3a40b998a490eb2811285a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [model-00002-of-00002.safetensors]:   0%|          | 0.00/3.26G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0bc1473984848d3ba22ac63dea55406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [pytorch_model-00001-of-00003.bin]:   0%|          | 0.00/4.60G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f9ce51d85346ca9a1c8c5c43bc9a39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [generation_config.json]:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca2c737db28e46cbb50f46fe59da2f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [model.safetensors.index.json]:   0%|          | 0.00/24.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bff4ef5a7d46405f8bd808f2bfda9c62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [pytorch_model-00002-of-00003.bin]:   0%|          | 0.00/4.61G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ec4e36dee64a7ba6aa6801d0393aaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [pytorch_model-00003-of-00003.bin]:   0%|          | 0.00/3.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3435781974e454590a32eec2e493b03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [pytorch_model.bin.index.json]:   0%|          | 0.00/23.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d7eaf95dc384e01b22cedbc0a45ee3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [README.md]:   0%|          | 0.00/6.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9243dcd02ebf40919ef94a5259fe973e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [tokenizer.model]:   0%|          | 0.00/488k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87bf320f785343d0aaf585c828b87af8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [USE_POLICY.md]:   0%|          | 0.00/4.68k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"AI-ModelScope/CodeLlama-7b-Instruct-hf\"\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    model.eval()\n",
    "except Exception as e:\n",
    "    logger.error(f\"Model loading failed: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# model_name = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     quantization_config=quant_config,\n",
    "#     device_map=\"auto\",\n",
    "#     attn_implementation=\"flash_attention_2\",\n",
    "#     torch_dtype=torch.float16,\n",
    "#     trust_remote_code=True\n",
    "# )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bfae2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a Python function to reverse a string. The function should take one parameter: the string to be reversed. The function should return the reversed version of the input string.\n",
      "\n",
      "**Example**:\n",
      "```python\n",
      "# Example usage\n",
      "print(reverse_string(\"hello\"))  # Output: \"olleh\"\n",
      "```\n",
      "\n",
      "**Solution**:\n",
      "```python\n",
      "def reverse_string(s):\n",
      "    \"\"\"\n",
      "    Reverse a given string and return the reversed version.\n",
      "    \n",
      "    Parameters:\n",
      "    s (str): The string to be reversed.\n",
      "    \n",
      "    Returns:\n",
      "    str: The\n"
     ]
    }
   ],
   "source": [
    "# Prepare input prompt\n",
    "prompt = \"Write a Python function to reverse a string.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate output\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "# Decode and print response\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8248f569",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = AIGCodeSet(cache_dir='../../data').get_dataset(split=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2b9963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# coding: utf-8\n",
      "# Your code here!\n",
      "import numpy as np\n",
      "\n",
      "n = input()\n",
      "m = input().strip().split()\n",
      "l = [0] * 100000\n",
      "k = [0] * 100000\n",
      "b = m[0::2]\n",
      "c = m[1::2]\n",
      "\n",
      "for i in b:\n",
      "    i = int(i)\n",
      "    l[i] = l[i] + 1\n",
      "\n",
      "for j in c:\n",
      "    j = int(j)\n",
      "    k[j] = k[j] + 1\n",
      "\n",
      "print(len(b)-int(max(l))+len(c)-int(max(k)))\n",
      "\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(train[1]['code'])\n",
    "print(train[1]['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792734e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how are you? I'm an AI language model created by OpenAI. I don't have feelings or emotions, but I'm here to help answer your questions and provide information on a wide range of topics.\n",
      "Is there anything specific you'd like to talk about or ask me? I'm happy to assist with any questions you may have!\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    \"Hello, how are you?\",\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    ").to(device)\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=512,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    num_return_sequences=1,\n",
    ")\n",
    "outputs = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d42d114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refine this code: # coding: utf-8\n",
      "# Your code here!\n",
      "import numpy as np\n",
      "\n",
      "n = input()\n",
      "m = input().strip().split()\n",
      "l = [0] * 100000\n",
      "k = [0] * 100000\n",
      "b = m[0::2]\n",
      "c = m[1::2]\n",
      "\n",
      "for i in b:\n",
      "    i = int(i)\n",
      "    l[i] = l[i] + 1\n",
      "\n",
      "for j in c:\n",
      "    j = int(j)\n",
      "    k[j] = k[j] + 1\n",
      "\n",
      "print(len(b)-int(max(l))+len(c)-int(max(k)))\n",
      "\n",
      "Please return only the refined code and nothing else. Here are some rules to follow:\n",
      "\n",
      "1. You should use list comprehension where possible.\n",
      "2. You should avoid using loops where possible.\n",
      "3. You should use built-in functions where appropriate.\n",
      "4. You should avoid hardcoding values like 100000 where possible.\n",
      "\n",
      "Here is the original code for reference:\n",
      "\n",
      "```python\n",
      "# coding: utf-8\n",
      "# Your code here!\n",
      "import numpy as np\n",
      "\n",
      "n = input()\n",
      "m = input().strip().split()\n",
      "l = [0] * 100000\n",
      "k = [0] * 100000\n",
      "b = m[0::2]\n",
      "c = m[1::2]\n",
      "\n",
      "for i in b:\n",
      "    i = int(i)\n",
      "    l[i] = l[i] + 1\n",
      "\n",
      "for j in c:\n",
      "    j = int(j)\n",
      "    k[j] = k[j] + 1\n",
      "\n",
      "print(len(b)-int(max(l))+len(c)-int(max(k)))\n",
      "```\n",
      "\n",
      "```python\n",
      "# coding: utf-8\n",
      "# Your code here!\n",
      "import numpy as np\n",
      "\n",
      "n = input()\n",
      "m = input().strip().split()\n",
      "b = m[0::2]\n",
      "c = m[1::2]\n",
      "\n",
      "l = [b.count(str(i)) for i in range(1, max(b) + 1)]\n",
      "k = [c.count(str(i)) for i in range(1, max(c) + 1)]\n",
      "\n",
      "print(len(b) - max(l) + len(c) - max(k))\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    f\"Refine this code: {train[1]['code']}\\nPlease return only the refined code and nothing else\",\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=1024,\n",
    ").to(device)\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=1024,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    num_return_sequences=1,\n",
    ")\n",
    "outputs = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08ef17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "n = input()\n",
    "m = input().strip().split()\n",
    "l = [0] * 100000\n",
    "k = [0] * 100000\n",
    "b = m[0::2]\n",
    "c = m[1::2]\n",
    "\n",
    "for i in b:\n",
    "    i = int(i)\n",
    "    l[i] = l[i] + 1\n",
    "\n",
    "for j in c:\n",
    "    j = int(j)\n",
    "    k[j] = k[j] + 1\n",
    "\n",
    "print(len(b)-int(max(l))+len(c)-int(max(k)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93b2634",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"int\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m b \u001b[38;5;241m=\u001b[39m m[\u001b[38;5;241m0\u001b[39m::\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m      6\u001b[0m c \u001b[38;5;241m=\u001b[39m m[\u001b[38;5;241m1\u001b[39m::\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m----> 8\u001b[0m l \u001b[38;5;241m=\u001b[39m [b\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;28mstr\u001b[39m(i)) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m)]\n\u001b[1;32m      9\u001b[0m k \u001b[38;5;241m=\u001b[39m [c\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;28mstr\u001b[39m(i)) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mmax\u001b[39m(c) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(b) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mmax\u001b[39m(l) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(c) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mmax\u001b[39m(k))\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"int\") to str"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "n = input()\n",
    "m = input().strip().split()\n",
    "b = m[0::2]\n",
    "c = m[1::2]\n",
    "\n",
    "l = [b.count(str(i)) for i in range(1, max(b) + 1)]\n",
    "k = [c.count(str(i)) for i in range(1, max(c) + 1)]\n",
    "\n",
    "print(len(b) - max(l) + len(c) - max(k))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
